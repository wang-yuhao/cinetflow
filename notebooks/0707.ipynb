{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Reference collector script for NetFlow v1, v5, and v9 Python package.\n",
    "This file belongs to https://github.com/bitkeks/python-netflow-v9-softflowd.\n",
    "Copyright 2016-2020 Dominik Pataky <software+pynetflow@dpataky.eu>\n",
    "Licensed under MIT License. See LICENSE.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import queue\n",
    "import socket\n",
    "import socketserver\n",
    "import threading\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "from package.ipfix import IPFIXTemplateNotRecognized\n",
    "#from package.utils import *\n",
    "from package.utils import UnknownExportVersion, parse_packet, flow_filter_v4, flow_filter_v6\n",
    "from package.v9 import V9TemplateNotRecognized\n",
    "from package.mysql_os import MysqlOperation\n",
    "from package.influxdb_os import InsertRecords\n",
    "\n",
    "\n",
    "RawPacket = namedtuple('RawPacket', ['ts', 'client', 'data'])\n",
    "ParsedPacket = namedtuple('ParsedPacket', ['ts', 'client', 'export'])\n",
    "\n",
    "# Amount of time to wait before dropping an undecodable ExportPacket\n",
    "PACKET_TIMEOUT = 60 * 60\n",
    "\n",
    "logger = logging.getLogger(\"netflow-collector\")\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "class QueuingRequestHandler(socketserver.BaseRequestHandler):\n",
    "    def handle(self):\n",
    "        data = self.request[0]  # get content, [1] would be the socket\n",
    "        self.server.queue.put(RawPacket(time.time(), self.client_address, data))\n",
    "        logger.debug(\n",
    "            \"Received %d bytes of data from %s\", len(data), self.client_address\n",
    "        )\n",
    "\n",
    "\n",
    "class QueuingUDPListener(socketserver.ThreadingUDPServer):\n",
    "    \"\"\"A threaded UDP server that adds a (time, data) tuple to a queue for\n",
    "    every request it sees\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, interface, queue):\n",
    "        self.queue = queue\n",
    "\n",
    "        # If IPv6 interface addresses are used, override the default AF_INET family\n",
    "        if \":\" in interface[0]:\n",
    "            self.address_family = socket.AF_INET6\n",
    "\n",
    "        super().__init__(interface, QueuingRequestHandler)\n",
    "\n",
    "\n",
    "class ThreadedNetFlowListener(threading.Thread):\n",
    "    \"\"\"A thread that listens for incoming NetFlow packets, processes them, and\n",
    "    makes them available to consumers.\n",
    "    - When initialized, will start listening for NetFlow packets on the provided\n",
    "      host and port and queuing them for processing.\n",
    "    - When started, will start processing and parsing queued packets.\n",
    "    - When stopped, will shut down the listener and stop processing.\n",
    "    - When joined, will wait for the listener to exit\n",
    "    For example, a simple script that outputs data until killed with CTRL+C:\n",
    "    >>> listener = ThreadedNetFlowListener('0.0.0.0', 2055)\n",
    "    >>> print(\"Listening for NetFlow packets\")\n",
    "    >>> listener.start() # start processing packets\n",
    "    >>> try:\n",
    "    ...     while True:\n",
    "    ...         ts, export = listener.get()\n",
    "    ...         print(\"Time: {}\".format(ts))\n",
    "    ...         for f in export.flows:\n",
    "    ...             print(\" - {IPV4_SRC_ADDR} sent data to {IPV4_DST_ADDR}\"\n",
    "    ...                   \"\".format(**f))\n",
    "    ... finally:\n",
    "    ...     print(\"Stopping...\")\n",
    "    ...     listener.stop()\n",
    "    ...     listener.join()\n",
    "    ...     print(\"Stopped!\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, host: str, port: int):\n",
    "        logger.info(\"Starting the NetFlow listener on {}:{}\".format(host, port))\n",
    "        self.output = queue.Queue()\n",
    "        self.input = queue.Queue()\n",
    "        self.server = QueuingUDPListener((host, port), self.input)\n",
    "        self.thread = threading.Thread(target=self.server.serve_forever)\n",
    "        self.thread.start()\n",
    "        self._shutdown = threading.Event()\n",
    "        super().__init__()\n",
    "\n",
    "    def get(self, block=True, timeout=None) -> ParsedPacket:\n",
    "        \"\"\"Get a processed flow.\n",
    "        If optional args 'block' is true and 'timeout' is None (the default),\n",
    "        block if necessary until a flow is available. If 'timeout' is\n",
    "        a non-negative number, it blocks at most 'timeout' seconds and raises\n",
    "        the queue.Empty exception if no flow was available within that time.\n",
    "        Otherwise ('block' is false), return a flow if one is immediately\n",
    "        available, else raise the queue.Empty exception ('timeout' is ignored\n",
    "        in that case).\n",
    "        \"\"\"\n",
    "        return self.output.get(block, timeout)\n",
    "\n",
    "    def run(self):\n",
    "        # Process packets from the queue\n",
    "        try:\n",
    "            templates = {\"netflow\": {}, \"ipfix\": {}}\n",
    "            to_retry = []\n",
    "            while not self._shutdown.is_set():\n",
    "                try:\n",
    "                    # 0.5s delay to limit CPU usage while waiting for new packets\n",
    "                    pkt = self.input.get(block=True, timeout=0.5)  # type: RawPacket\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # templates is passed as reference, updated in V9ExportPacket\n",
    "                    export = parse_packet(pkt.data, templates)\n",
    "                except UnknownExportVersion as e:\n",
    "                    logger.error(\"%s, ignoring the packet\", e)\n",
    "                    continue\n",
    "                except (V9TemplateNotRecognized, IPFIXTemplateNotRecognized):\n",
    "                    # TODO: differentiate between v9 and IPFIX, use separate to_retry lists\n",
    "                    if time.time() - pkt.ts > PACKET_TIMEOUT:\n",
    "                        logger.warning(\"Dropping an old and undecodable v9/IPFIX ExportPacket\")\n",
    "                    else:\n",
    "                        to_retry.append(pkt)\n",
    "                        logger.debug(\"Failed to decode a v9/IPFIX ExportPacket - will \"\n",
    "                                     \"re-attempt when a new template is discovered\")\n",
    "                    continue\n",
    "\n",
    "                if export.header.version == 10:\n",
    "                    logger.debug(\"Processed an IPFIX ExportPacket with length %d.\", export.header.length)\n",
    "                else:\n",
    "                    logger.debug(\"Processed a v%d ExportPacket with %d flows.\",\n",
    "                                 export.header.version, export.header.count)\n",
    "\n",
    "                # If any new templates were discovered, dump the unprocessable\n",
    "                # data back into the queue and try to decode them again\n",
    "                if export.header.version in [9, 10] and export.contains_new_templates and to_retry:\n",
    "                    logger.debug(\"Received new template(s)\")\n",
    "                    logger.debug(\"Will re-attempt to decode %d old v9/IPFIX ExportPackets\", len(to_retry))\n",
    "                    for p in to_retry:\n",
    "                        self.input.put(p)\n",
    "                    to_retry.clear()\n",
    "\n",
    "                self.output.put(ParsedPacket(pkt.ts, pkt.client, export))\n",
    "        finally:\n",
    "            # Only reached when while loop ends\n",
    "            self.server.shutdown()\n",
    "            self.server.server_close()\n",
    "\n",
    "    def stop(self):\n",
    "        logger.info(\"Shutting down the NetFlow listener\")\n",
    "        self._shutdown.set()\n",
    "\n",
    "    def join(self, timeout=None):\n",
    "        self.thread.join(timeout=timeout)\n",
    "        super().join(timeout=timeout)\n",
    "\n",
    "\n",
    "def get_export_packets(host: str, port: int) -> ParsedPacket:\n",
    "    \"\"\"A threaded generator that will yield ExportPacket objects until it is killed\n",
    "    \"\"\"\n",
    "    listener = ThreadedNetFlowListener(host, port)\n",
    "    listener.start()\n",
    "    try:\n",
    "        while True:\n",
    "            yield listener.get()\n",
    "    finally:\n",
    "        listener.stop()\n",
    "        listener.join()\n",
    "        \n",
    "async def main(*flows):\n",
    "    myTool = MysqlOperation()\n",
    "    await asyncio.gather(\n",
    "        \n",
    "        myTool.insertRecords(*flows)\n",
    "    )\n",
    "\n",
    "if __name__ == \"netflow.collector\":\n",
    "    logger.error(\"The collector is currently meant to be used as a CLI tool only.\")\n",
    "    logger.error(\"Use 'python3 -m netflow.collector -h' in your console for additional help.\")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "        # With every parsed flow a new line is appended to the output file. In previous versions, this was implemented\n",
    "        # by storing the whole data dict in memory and dumping it regularly onto disk. This was extremely fragile, as\n",
    "        # it a) consumed a lot of memory and CPU (dropping packets since storing one flow took longer than the arrival\n",
    "        # of the next flow) and b) broke the exported JSON file, if the collector crashed during the write process,\n",
    "        # rendering all collected flows during the runtime of the collector useless (the file contained one large JSON\n",
    "        # dict which represented the 'data' dict).\n",
    "\n",
    "        # In this new approach, each received flow is parsed as usual, but it gets appended to a gzipped file each time.\n",
    "        # All in all, this improves in three aspects:InnodbOperation\n",
    "        # 1. collected flow data is not stored in memory any more\n",
    "        # 2. received and parsed flows are persisted reliably\n",
    "        # 3. the disk usage of files with JSON and its full strings as keys is reduced by using gzipped files\n",
    "        # This also means that the files have to be handled differently, because they are gzipped and not formatted as\n",
    "        # one single big JSON dump, but rather many little JSON dumps, separated by line breaks.\n",
    "        flows = []\n",
    "        for ts, client, export in get_export_packets(\"0.0.0.0\", 9996):\n",
    "            try: \n",
    "                export.flows[0].IP_PROTOCOL_VERSION\n",
    "            except AttributeError:\n",
    "                flow = flow_filter_v4(client, export)\n",
    "            else:\n",
    "                if(export.flows[0].IP_PROTOCOL_VERSION == 4):\n",
    "                    flow = flow_filter_v4(client, export)\n",
    "                else:\n",
    "                    flow = flow_filter_v6(client, export)\n",
    "            print(flow)\n",
    "            flows.append(flow)\n",
    "            if(len(flows) == 10000): \n",
    "                myTool = MysqlOperation()\n",
    "                #innoTool = InnodbOperation()\n",
    "                asyncio.run(asyncio.gather(myTool.insertRecords(*flows), InsertRecords(*flows)))\n",
    "                flows = []\n",
    "\n",
    "\n",
    "            \n",
    "                \n",
    "# (srcaddr, srcport, first, last, protocol, flows, packets, bytes)\n",
    "# TODO change the format of arcaddr to varbinary\n",
    "                \n",
    "                \n",
    "            #entry = {ts: {\n",
    "            #    \"client\": client,\n",
    "            #    \"header\": export.header.to_dict(),\n",
    "            #    \"flows\": [flow.data for flow in export.flows]}\n",
    "            #}\n",
    "        line = json.dumps(entry).encode() + b\"\\n\"  # byte encoded line\n",
    "        with gzip.open(\"{}.gz\".format(int(time.time())), \"ab\") as fh:  # open as append, not reading the whole file\n",
    "                fh.write(line)\n",
    "except KeyboardInterrupt:\n",
    "        logger.info(\"Received KeyboardInterrupt, passing through\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aioinflux\n",
      "  Using cached aioinflux-0.9.0-py3-none-any.whl (16 kB)\n",
      "Collecting ciso8601\n",
      "  Using cached ciso8601-2.1.3.tar.gz (15 kB)\n",
      "Collecting aiohttp>=3.0\n",
      "  Downloading aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<4.0,>=2.0 in ./my_env/lib/python3.6/site-packages (from aiohttp>=3.0->aioinflux) (3.0.4)\n",
      "Collecting typing-extensions>=3.6.5; python_version < \"3.7\"\n",
      "  Downloading typing_extensions-3.7.4.2-py3-none-any.whl (22 kB)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<5.0,>=4.5\n",
      "  Downloading multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (252 kB)\n",
      "\u001b[K     |████████████████████████████████| 252 kB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in ./my_env/lib/python3.6/site-packages (from aiohttp>=3.0->aioinflux) (19.3.0)\n",
      "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "Requirement already satisfied: idna>=2.0 in ./my_env/lib/python3.6/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.0->aioinflux) (2.9)\n",
      "Building wheels for collected packages: ciso8601, idna-ssl\n",
      "  Building wheel for ciso8601 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ciso8601: filename=ciso8601-2.1.3-cp36-cp36m-linux_x86_64.whl size=28295 sha256=caca3eb801caf532b5e6a1202c45331de0b981b70dad02ec70114e5ee8da6521\n",
      "  Stored in directory: /home/yuhao/.cache/pip/wheels/26/2a/fd/663a2fcd09041c42fd9dd5bdca263f417bb19dcca148733038\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=25a467c357ee2b34756d0a3fbec469a1c8f358d362acbc6996168b5b4a194b31\n",
      "  Stored in directory: /home/yuhao/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "Successfully built ciso8601 idna-ssl\n",
      "Installing collected packages: ciso8601, typing-extensions, async-timeout, multidict, yarl, idna-ssl, aiohttp, aioinflux\n",
      "Successfully installed aiohttp-3.6.2 aioinflux-0.9.0 async-timeout-3.0.1 ciso8601-2.1.3 idna-ssl-1.1.0 multidict-4.7.6 typing-extensions-3.7.4.2 yarl-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install aioinflux\n",
    "#!pip install aiomysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'get-pip.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!python get-pip.py pip==19.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            print(export)\n",
    "            for flow in export.flows:\n",
    "                try:\n",
    "                    flows.IPV4_SRC_ADDR\n",
    "                except NameError:\n",
    "                    srcaddr = 0\n",
    "                \n",
    "                 # convert string ip to int ip for save storage space   \n",
    "                if isinstance(flow.IPV4_SRC_ADDR, str):\n",
    "                    srcaddr = str_ip_to_int(flow.IPV4_SRC_ADDR)\n",
    "                else:\n",
    "                    srcaddr = flow.IPV4_SRC_ADDR\n",
    "                \n",
    "                srcport = flow.L4_SRC_PORT\n",
    "                print(srcaddr)\n",
    "                print(flow)\n",
    "                print(type(flow))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srcaddr</th>\n",
       "      <th>srcport</th>\n",
       "      <th>First</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2130706433</td>\n",
       "      <td>7788</td>\n",
       "      <td>4214989963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2130706433</td>\n",
       "      <td>7788</td>\n",
       "      <td>12108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      srcaddr  srcport       First\n",
       "0  2130706433     7788  4214989963\n",
       "1  2130706433     7788       12108"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "l1 = [(2130706433, 7788, 4214989963, 4215020796, 6, 10, 208, 164207),(2130706433, 7788, 12108, 57372, 1, 3, 21, 1849)]\n",
    "# new_record = pd.DataFrame(data=l1)\n",
    "new_record = pd.DataFrame(data=l1, columns=['srcaddr','srcport','First','last', 'protocol', 'flows', 'packets', 'bytes'])\n",
    "new_record = pd.DataFrame(data=new_record, columns=['srcaddr','srcport','First'])\n",
    "\n",
    "new_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create database: assetdb_ts\n",
      "Write DataFrame\n",
      "Client Library Write: 21.009989887999836s\n",
      "Read DataFrame\n",
      "Client Library Write: 17.62514780899801s\n",
      "Result: defaultdict(<class 'list'>, {'demo':                                 srcaddr  srcport\n",
      "2020-05-28 14:07:31+00:00     0.0.126.9     2770\n",
      "2020-05-28 14:07:31+00:00   0.0.133.115    65021\n",
      "2020-05-28 14:07:31+00:00   0.0.158.146    16787\n",
      "2020-05-28 14:07:31+00:00   0.0.161.231    37220\n",
      "2020-05-28 14:07:31+00:00    0.0.17.137     9037\n",
      "...                                 ...      ...\n",
      "2020-05-28 14:07:31+00:00  99.99.47.121     7066\n",
      "2020-05-28 14:07:31+00:00   99.99.57.13    38140\n",
      "2020-05-28 14:07:31+00:00   99.99.74.45    54016\n",
      "2020-05-28 14:07:31+00:00  99.99.85.152    41382\n",
      "2020-05-28 14:07:31+00:00   99.99.9.222    25293\n",
      "\n",
      "[999867 rows x 2 columns]})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tutorial for using pandas and the InfluxDB client.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "from influxdb import DataFrameClient\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Instantiate the connection to the InfluxDB client.\"\"\"\n",
    "    user = 'root'\n",
    "    password = '0318'\n",
    "    dbname = 'assetdb_ts'\n",
    "    protocol = 'line'\n",
    "\n",
    "    client = DataFrameClient('localhost', 8086, user, password, dbname)\n",
    "\n",
    "    with open('data/netflow/' + '20200528140758.json') as record:\n",
    "        new_record = json.load(record)\n",
    "\n",
    "        new_record = pd.DataFrame(data=new_record,                       \n",
    "                                  columns=['srcaddr','srcport','First'])\n",
    "        new_record['First'] = pd.to_datetime(new_record['First'], format='%Y%m%d%H%M%S')\n",
    "        new_record.set_index('First', inplace=True)\n",
    "\n",
    "\n",
    "        print(\"Create database: \" + dbname)\n",
    "        client.create_database(dbname)\n",
    "\n",
    "        print(\"Write DataFrame\")\n",
    "        client_write_start_time = time.perf_counter()\n",
    "        client.write_points(new_record, 'demo', tag_columns=['srcaddr'], time_precision='ms', batch_size=10000, protocol=protocol)\n",
    "        client_write_end_time = time.perf_counter()\n",
    "        print(\"Client Library Write: {time}s\".format(time=client_write_end_time - client_write_start_time))\n",
    "        \n",
    "        \n",
    "        print(\"Read DataFrame\")\n",
    "        client_write_start_time = time.perf_counter()\n",
    "        result = client.query(\"select * from demo\")\n",
    "        client_write_end_time = time.perf_counter()\n",
    "        print(\"Client Library Write: {time}s\".format(time=client_write_end_time - client_write_start_time))\n",
    "        print(\"Result: {0}\".format(result))\n",
    "        #print(\"Delete database: \" + dbname)\n",
    "        #client.drop_database(dbname)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130706433\n",
      "127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "import socket  \n",
    "import struct  \n",
    "  \n",
    "\n",
    "ip = '127.0.0.1'  \n",
    "int_ip = struct.unpack('!I', socket.inet_aton(ip))[0]  \n",
    "print(int_ip)  \n",
    "str_ip = socket.inet_ntoa(struct.pack('!I', int_ip))  \n",
    "print(str_ip) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create database: assetdb_ts\n",
      "Write DataFrame\n",
      "Client Library Write: 20.290647964997333s\n",
      "Read DataFrame\n",
      "Client Library Write: 16.82773450500099s\n",
      "Result: defaultdict(<class 'list'>, {'demo':                               srcaddr  srcport\n",
      "2020-05-28 14:07:31+00:00  1000000817    41026\n",
      "2020-05-28 14:07:31+00:00  1000003568    28786\n",
      "2020-05-28 14:07:31+00:00  1000004532    20217\n",
      "2020-05-28 14:07:31+00:00  1000010964    37568\n",
      "2020-05-28 14:07:31+00:00  1000013317    37817\n",
      "...                               ...      ...\n",
      "2020-05-28 14:07:31+00:00   999972090    54473\n",
      "2020-05-28 14:07:31+00:00   999983515    36537\n",
      "2020-05-28 14:07:31+00:00   999984902     7682\n",
      "2020-05-28 14:07:31+00:00   999991344    23997\n",
      "2020-05-28 14:07:31+00:00   999999357    62128\n",
      "\n",
      "[999867 rows x 2 columns]})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tutorial for using pandas and the InfluxDB client.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "from influxdb import DataFrameClient\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Instantiate the connection to the InfluxDB client.\"\"\"\n",
    "    user = 'root'\n",
    "    password = '0318'\n",
    "    dbname = 'assetdb_ts'\n",
    "    protocol = 'line'\n",
    "\n",
    "    client = DataFrameClient('localhost', 8086, user, password, dbname)\n",
    "\n",
    "    with open('data/netflow/' + '20200528140758.json') as record:\n",
    "        new_record = json.load(record)\n",
    "\n",
    "        new_record = pd.DataFrame(data=new_record,                       \n",
    "                                  columns=['srcaddr','srcport','First'])\n",
    "        new_record[\"srcaddr\"] = new_record[\"srcaddr\"].map(lambda x: struct.unpack('!I', socket.inet_aton(x))[0])\n",
    "        #new_record[\"srcaddr\"] = struct.unpack('!I', socket.inet_aton(new_record[\"srcaddr\"]))[0]\n",
    "        new_record['First'] = pd.to_datetime(new_record['First'], format='%Y%m%d%H%M%S')\n",
    "        new_record.set_index('First', inplace=True)\n",
    "\n",
    "\n",
    "        print(\"Create database: \" + dbname)\n",
    "        client.create_database(dbname)\n",
    "\n",
    "        print(\"Write DataFrame\")\n",
    "        client_write_start_time = time.perf_counter()\n",
    "        client.write_points(new_record, 'demo', tag_columns=['srcaddr'], time_precision='ms', batch_size=10000, protocol=protocol)\n",
    "        client_write_end_time = time.perf_counter()\n",
    "        print(\"Client Library Write: {time}s\".format(time=client_write_end_time - client_write_start_time))\n",
    "        \n",
    "        \n",
    "        print(\"Read DataFrame\")\n",
    "        client_write_start_time = time.perf_counter()\n",
    "        result = client.query(\"select * from demo\")\n",
    "        client_write_end_time = time.perf_counter()\n",
    "        print(\"Client Library Write: {time}s\".format(time=client_write_end_time - client_write_start_time))\n",
    "        print(\"Result: {0}\".format(result))\n",
    "        #print(\"Delete database: \" + dbname)\n",
    "        #client.drop_database(dbname)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete database: assetdb_ts\n"
     ]
    }
   ],
   "source": [
    "user = 'root'\n",
    "password = '0318'\n",
    "dbname = 'assetdb_ts'\n",
    "protocol = 'line'\n",
    "\n",
    "client = DataFrameClient('localhost', 8086, user, password, dbname)\n",
    "print(\"Delete database: \" + dbname)\n",
    "client.drop_database(dbname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-66-cca0c3453d63>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-cca0c3453d63>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    \"\"\"Instantiate the connection to the InfluxDB client.\"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tutorial for using pandas and the InfluxDB client.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "time_write = []\n",
    "time_query = []\n",
    "\n",
    "from influxdb import DataFrameClient\n",
    "\n",
    "    \"\"\"Instantiate the connection to the InfluxDB client.\"\"\"\n",
    "    user = 'root'\n",
    "    password = '0318'\n",
    "    dbname = 'assetdb_ts'\n",
    "    protocol = 'line'\n",
    "\n",
    "    client = DataFrameClient('localhost', 8086, user, password, dbname)\n",
    "\n",
    "    for i in range(10):\n",
    "        with open('data/netflow/' + '20200528140758.json') as record:\n",
    "            new_record = json.load(record)\n",
    "\n",
    "            new_record = pd.DataFrame(data=new_record,                       \n",
    "                                      columns=['srcaddr','srcport','First'])\n",
    "            new_record['First'] = pd.to_datetime(new_record['First'], format='%Y%m%d%H%M%S')\n",
    "            new_record.set_index('First', inplace=True)\n",
    "\n",
    "\n",
    "            print(\"Create database: \" + dbname)\n",
    "            client.create_database(dbname)\n",
    "\n",
    "            print(\"Write DataFrame\")\n",
    "            client_write_start_time = time.perf_counter()\n",
    "            client.write_points(new_record, 'demo', tag_columns=['srcaddr'], time_precision='ms', batch_size=10000, protocol=protocol)\n",
    "            client_write_end_time = time.perf_counter()\n",
    "            print(\"Client Library Write: {time}s\".format(time=client_write_end_time - client_write_start_time))\n",
    "            time_write.append(client_write_end_time - client_write_start_time)\n",
    "\n",
    "            print(\"Read DataFrame\")\n",
    "            client_query_start_time = time.perf_counter()\n",
    "            result = client.query(\"select * from demo\")\n",
    "            client_query_end_time = time.perf_counter()\n",
    "            print(\"Client Library Write: {time}s\".format(time=client_query_end_time - client_query_start_time))\n",
    "            time_query.append(client_write_end_time - client_write_start_time)\n",
    "            print(\"Result: {0}\".format(result))\n",
    "            print(\"Delete database: \" + dbname)\n",
    "            client.drop_database(dbname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200528140758.json\n",
      "20200528141325.json\n",
      "20200528141851.json\n",
      "20200528142416.json\n",
      "20200528142940.json\n",
      "20200528143504.json\n",
      "20200528144029.json\n",
      "20200528144554.json\n",
      "20200528145119.json\n",
      "20200528145643.json\n",
      "20200528150208.json\n",
      "20200528150732.json\n",
      "20200528151257.json\n",
      "20200528151821.json\n",
      "20200528152346.json\n",
      "20200528152910.json\n",
      "20200528153435.json\n",
      "20200528153959.json\n",
      "20200528154524.json\n",
      "20200528155048.json\n",
      "20200528155613.json\n",
      "20200528160137.json\n",
      "20200528160702.json\n",
      "20200528161228.json\n",
      "20200528161752.json\n",
      "20200528162317.json\n",
      "20200528162841.json\n",
      "20200528163405.json\n",
      "20200528163930.json\n",
      "20200528164454.json\n",
      "20200528165019.json\n",
      "20200528165543.json\n",
      "20200528170107.json\n",
      "20200528170632.json\n",
      "20200528171156.json\n",
      "20200528171721.json\n",
      "20200528172246.json\n",
      "20200528172810.json\n",
      "20200528173335.json\n",
      "20200528173859.json\n",
      "20200528174424.json\n",
      "20200528174948.json\n",
      "20200528175513.json\n",
      "20200528180037.json\n",
      "20200528180602.json\n",
      "20200528181126.json\n",
      "20200528181650.json\n",
      "20200528182215.json\n",
      "20200528182740.json\n",
      "20200528183304.json\n",
      "20200528183829.json\n",
      "20200528184353.json\n",
      "20200528184917.json\n",
      "20200528185442.json\n",
      "20200528190006.json\n",
      "20200528190530.json\n",
      "20200528191055.json\n",
      "20200528191619.json\n",
      "20200528192143.json\n",
      "20200528192708.json\n",
      "20200528193232.json\n",
      "20200528193757.json\n",
      "20200528194321.json\n",
      "20200528194845.json\n",
      "20200528195410.json\n",
      "20200528195934.json\n",
      "20200528200458.json\n",
      "20200528201023.json\n",
      "20200528201547.json\n",
      "20200528202112.json\n",
      "20200528202636.json\n",
      "20200528203201.json\n",
      "20200528203725.json\n",
      "20200528204250.json\n",
      "20200528204814.json\n",
      "20200528205338.json\n",
      "20200528205903.json\n",
      "20200528210427.json\n",
      "20200528210951.json\n",
      "20200528211516.json\n",
      "20200528212040.json\n",
      "20200528212604.json\n",
      "20200528213129.json\n",
      "20200528213653.json\n",
      "20200528214217.json\n",
      "20200528214742.json\n",
      "20200528215306.json\n",
      "20200528215830.json\n",
      "20200528220354.json\n",
      "20200528220919.json\n",
      "20200528221443.json\n",
      "20200528222007.json\n",
      "20200528222532.json\n",
      "20200528223056.json\n",
      "20200528223620.json\n",
      "20200528224144.json\n",
      "20200528224709.json\n",
      "20200528225233.json\n",
      "20200528225758.json\n",
      "20200528230322.json\n",
      "20200528230846.json\n",
      "20200528231411.json\n",
      "20200528231935.json\n",
      "20200528232500.json\n",
      "20200528233025.json\n",
      "20200528233549.json\n",
      "20200528234113.json\n",
      "20200528234638.json\n",
      "20200528235202.json\n",
      "20200528235727.json\n",
      "20200529000251.json\n",
      "20200529000815.json\n",
      "20200529001340.json\n",
      "20200529001904.json\n",
      "20200529002428.json\n",
      "20200529002953.json\n",
      "20200529003517.json\n",
      "20200529004042.json\n",
      "20200529004606.json\n",
      "20200529005131.json\n",
      "20200529005655.json\n",
      "20200529010219.json\n",
      "20200529010744.json\n",
      "20200529011308.json\n",
      "20200529011832.json\n",
      "20200529012357.json\n",
      "20200529012921.json\n",
      "20200529013445.json\n",
      "20200529014010.json\n",
      "20200529014534.json\n",
      "20200529015058.json\n",
      "20200529015622.json\n",
      "20200529020147.json\n",
      "20200529020711.json\n",
      "20200529021237.json\n",
      "20200529021801.json\n",
      "20200529022325.json\n",
      "20200529022850.json\n",
      "20200529023414.json\n",
      "20200529023938.json\n",
      "20200529024503.json\n",
      "20200529025027.json\n",
      "20200529025551.json\n",
      "20200529030116.json\n",
      "20200529030640.json\n",
      "20200529031205.json\n",
      "20200529031729.json\n",
      "20200529032256.json\n",
      "20200529032823.json\n",
      "20200529033349.json\n",
      "20200529033918.json\n",
      "20200529034444.json\n",
      "20200529035008.json\n",
      "20200529035533.json\n",
      "20200529040057.json\n",
      "20200529040622.json\n",
      "20200529041147.json\n",
      "20200529041711.json\n",
      "20200529042236.json\n",
      "20200529042800.json\n",
      "20200529043325.json\n",
      "20200529043849.json\n",
      "20200529044414.json\n",
      "20200529044939.json\n",
      "20200529045503.json\n",
      "20200529050027.json\n",
      "20200529050552.json\n",
      "20200529051117.json\n",
      "20200529051641.json\n",
      "20200529052206.json\n",
      "20200529052730.json\n",
      "20200529053255.json\n",
      "20200529053819.json\n",
      "20200529054344.json\n",
      "20200529054908.json\n",
      "20200529055433.json\n",
      "20200529055958.json\n",
      "20200529060522.json\n",
      "20200529061047.json\n",
      "20200529061611.json\n",
      "20200529062136.json\n",
      "20200529062700.json\n",
      "20200529063225.json\n",
      "20200529063749.json\n",
      "20200529064314.json\n",
      "20200529064838.json\n",
      "20200529065403.json\n",
      "20200529065927.json\n",
      "20200529070452.json\n",
      "20200529071016.json\n",
      "20200529071540.json\n",
      "20200529072105.json\n",
      "20200529072629.json\n",
      "20200529073154.json\n",
      "20200529073718.json\n",
      "20200529074242.json\n",
      "20200529074807.json\n",
      "20200529075331.json\n",
      "20200529075855.json\n",
      "20200529080420.json\n",
      "20200529080944.json\n",
      "20200529081508.json\n",
      "20200529082033.json\n",
      "20200529082557.json\n",
      "20200529083122.json\n",
      "20200529083646.json\n",
      "20200529084210.json\n",
      "20200529084735.json\n",
      "20200529085259.json\n",
      "20200529085824.json\n",
      "20200529090348.json\n",
      "20200529090913.json\n",
      "20200529091437.json\n",
      "20200529092001.json\n",
      "20200529092526.json\n",
      "20200529093050.json\n",
      "20200529093614.json\n",
      "20200529094139.json\n",
      "20200529094703.json\n",
      "20200529095228.json\n",
      "20200529095752.json\n",
      "20200529100317.json\n",
      "20200529100841.json\n",
      "20200529101406.json\n",
      "20200529101930.json\n",
      "20200529102454.json\n",
      "20200529103019.json\n",
      "20200529103543.json\n",
      "20200528140758.json\n",
      "20200528141325.json\n",
      "20200528141851.json\n",
      "20200528142416.json\n",
      "20200528142940.json\n",
      "20200528143504.json\n",
      "20200528144029.json\n",
      "20200528144554.json\n",
      "20200528145119.json\n",
      "20200528145643.json\n",
      "20200528150208.json\n",
      "20200528150732.json\n",
      "20200528151257.json\n",
      "20200528151821.json\n",
      "20200528152346.json\n",
      "20200528152910.json\n",
      "20200528153435.json\n",
      "20200528153959.json\n",
      "20200528154524.json\n",
      "20200528155048.json\n",
      "20200528155613.json\n",
      "20200528160137.json\n",
      "20200528160702.json\n",
      "20200528161228.json\n",
      "20200528161752.json\n",
      "20200528162317.json\n",
      "20200528162841.json\n",
      "20200528163405.json\n",
      "20200528163930.json\n",
      "20200528164454.json\n",
      "20200528165019.json\n",
      "20200528165543.json\n",
      "20200528170107.json\n",
      "20200528170632.json\n",
      "20200528171156.json\n",
      "20200528171721.json\n",
      "20200528172246.json\n",
      "20200528172810.json\n",
      "20200528173335.json\n",
      "20200528173859.json\n",
      "20200528174424.json\n",
      "20200528174948.json\n",
      "20200528175513.json\n",
      "20200528180037.json\n",
      "20200528180602.json\n",
      "20200528181126.json\n",
      "20200528181650.json\n",
      "20200528182215.json\n",
      "20200528182740.json\n",
      "20200528183304.json\n",
      "20200528183829.json\n",
      "20200528184353.json\n",
      "20200528184917.json\n",
      "20200528185442.json\n",
      "20200528190006.json\n",
      "20200528190530.json\n",
      "20200528191055.json\n",
      "20200528191619.json\n",
      "20200528192143.json\n",
      "20200528192708.json\n",
      "20200528193232.json\n",
      "20200528193757.json\n",
      "20200528194321.json\n",
      "20200528194845.json\n",
      "20200528195410.json\n",
      "20200528195934.json\n",
      "20200528200458.json\n",
      "20200528201023.json\n",
      "20200528201547.json\n",
      "20200528202112.json\n",
      "20200528202636.json\n",
      "20200528203201.json\n",
      "20200528203725.json\n",
      "20200528204250.json\n",
      "20200528204814.json\n",
      "20200528205338.json\n",
      "20200528205903.json\n",
      "20200528210427.json\n",
      "20200528210951.json\n",
      "20200528211516.json\n",
      "20200528212040.json\n",
      "20200528212604.json\n",
      "20200528213129.json\n",
      "20200528213653.json\n",
      "20200528214217.json\n",
      "20200528214742.json\n",
      "20200528215306.json\n",
      "20200528215830.json\n",
      "20200528220354.json\n",
      "20200528220919.json\n",
      "20200528221443.json\n",
      "20200528222007.json\n",
      "20200528222532.json\n",
      "20200528223056.json\n",
      "20200528223620.json\n",
      "20200528224144.json\n",
      "20200528224709.json\n",
      "20200528225233.json\n",
      "20200528225758.json\n",
      "20200528230322.json\n",
      "20200528230846.json\n",
      "20200528231411.json\n",
      "20200528231935.json\n",
      "20200528232500.json\n",
      "20200528233025.json\n",
      "20200528233549.json\n",
      "20200528234113.json\n",
      "20200528234638.json\n",
      "20200528235202.json\n",
      "20200528235727.json\n",
      "20200529000251.json\n",
      "20200529000815.json\n",
      "20200529001340.json\n",
      "20200529001904.json\n",
      "20200529002428.json\n",
      "20200529002953.json\n",
      "20200529003517.json\n",
      "20200529004042.json\n",
      "20200529004606.json\n",
      "20200529005131.json\n",
      "20200529005655.json\n",
      "20200529010219.json\n",
      "20200529010744.json\n",
      "20200529011308.json\n",
      "20200529011832.json\n",
      "20200529012357.json\n",
      "20200529012921.json\n",
      "20200529013445.json\n",
      "20200529014010.json\n",
      "20200529014534.json\n",
      "20200529015058.json\n",
      "20200529015622.json\n",
      "20200529020147.json\n",
      "20200529020711.json\n",
      "20200529021237.json\n",
      "20200529021801.json\n",
      "20200529022325.json\n",
      "20200529022850.json\n",
      "20200529023414.json\n",
      "20200529023938.json\n",
      "20200529024503.json\n",
      "20200529025027.json\n",
      "20200529025551.json\n",
      "20200529030116.json\n",
      "20200529030640.json\n",
      "20200529031205.json\n",
      "20200529031729.json\n",
      "20200529032256.json\n",
      "20200529032823.json\n",
      "20200529033349.json\n",
      "20200529033918.json\n",
      "20200529034444.json\n",
      "20200529035008.json\n",
      "20200529035533.json\n",
      "20200529040057.json\n",
      "20200529040622.json\n",
      "20200529041147.json\n",
      "20200529041711.json\n",
      "20200529042236.json\n",
      "20200529042800.json\n",
      "20200529043325.json\n",
      "20200529043849.json\n",
      "20200529044414.json\n",
      "20200529044939.json\n",
      "20200529045503.json\n",
      "20200529050027.json\n",
      "20200529050552.json\n",
      "20200529051117.json\n",
      "20200529051641.json\n",
      "20200529052206.json\n",
      "20200529052730.json\n",
      "20200529053255.json\n",
      "20200529053819.json\n",
      "20200529054344.json\n",
      "20200529054908.json\n",
      "20200529055433.json\n",
      "20200529055958.json\n",
      "20200529060522.json\n",
      "20200529061047.json\n",
      "20200529061611.json\n",
      "20200529062136.json\n",
      "20200529062700.json\n",
      "20200529063225.json\n",
      "20200529063749.json\n",
      "20200529064314.json\n",
      "20200529064838.json\n",
      "20200529065403.json\n",
      "20200529065927.json\n",
      "20200529070452.json\n",
      "20200529071016.json\n",
      "20200529071540.json\n",
      "20200529072105.json\n",
      "20200529072629.json\n",
      "20200529073154.json\n",
      "20200529073718.json\n",
      "20200529074242.json\n",
      "20200529074807.json\n",
      "20200529075331.json\n",
      "20200529075855.json\n",
      "20200529080420.json\n",
      "20200529080944.json\n",
      "20200529081508.json\n",
      "20200529082033.json\n",
      "20200529082557.json\n",
      "20200529083122.json\n",
      "20200529083646.json\n",
      "20200529084210.json\n",
      "20200529084735.json\n",
      "20200529085259.json\n",
      "20200529085824.json\n",
      "20200529090348.json\n",
      "20200529090913.json\n",
      "20200529091437.json\n",
      "20200529092001.json\n",
      "20200529092526.json\n",
      "20200529093050.json\n",
      "20200529093614.json\n",
      "20200529094139.json\n",
      "20200529094703.json\n",
      "20200529095228.json\n",
      "20200529095752.json\n",
      "20200529100317.json\n",
      "20200529100841.json\n",
      "20200529101406.json\n",
      "20200529101930.json\n",
      "20200529102454.json\n",
      "20200529103019.json\n",
      "20200529103543.json\n",
      "['20200528140758.json', '20200528141325.json', '20200528141851.json', '20200528142416.json', '20200528142940.json', '20200528143504.json', '20200528144029.json', '20200528144554.json', '20200528145119.json', '20200528145643.json', '20200528150208.json', '20200528150732.json', '20200528151257.json', '20200528151821.json', '20200528152346.json', '20200528152910.json', '20200528153435.json', '20200528153959.json', '20200528154524.json', '20200528155048.json', '20200528155613.json', '20200528160137.json', '20200528160702.json', '20200528161228.json', '20200528161752.json', '20200528162317.json', '20200528162841.json', '20200528163405.json', '20200528163930.json', '20200528164454.json', '20200528165019.json', '20200528165543.json', '20200528170107.json', '20200528170632.json', '20200528171156.json', '20200528171721.json', '20200528172246.json', '20200528172810.json', '20200528173335.json', '20200528173859.json', '20200528174424.json', '20200528174948.json', '20200528175513.json', '20200528180037.json', '20200528180602.json', '20200528181126.json', '20200528181650.json', '20200528182215.json', '20200528182740.json', '20200528183304.json', '20200528183829.json', '20200528184353.json', '20200528184917.json', '20200528185442.json', '20200528190006.json', '20200528190530.json', '20200528191055.json', '20200528191619.json', '20200528192143.json', '20200528192708.json', '20200528193232.json', '20200528193757.json', '20200528194321.json', '20200528194845.json', '20200528195410.json', '20200528195934.json', '20200528200458.json', '20200528201023.json', '20200528201547.json', '20200528202112.json', '20200528202636.json', '20200528203201.json', '20200528203725.json', '20200528204250.json', '20200528204814.json', '20200528205338.json', '20200528205903.json', '20200528210427.json', '20200528210951.json', '20200528211516.json', '20200528212040.json', '20200528212604.json', '20200528213129.json', '20200528213653.json', '20200528214217.json', '20200528214742.json', '20200528215306.json', '20200528215830.json', '20200528220354.json', '20200528220919.json', '20200528221443.json', '20200528222007.json', '20200528222532.json', '20200528223056.json', '20200528223620.json', '20200528224144.json', '20200528224709.json', '20200528225233.json', '20200528225758.json', '20200528230322.json', '20200528230846.json', '20200528231411.json', '20200528231935.json', '20200528232500.json', '20200528233025.json', '20200528233549.json', '20200528234113.json', '20200528234638.json', '20200528235202.json', '20200528235727.json', '20200529000251.json', '20200529000815.json', '20200529001340.json', '20200529001904.json', '20200529002428.json', '20200529002953.json', '20200529003517.json', '20200529004042.json', '20200529004606.json', '20200529005131.json', '20200529005655.json', '20200529010219.json', '20200529010744.json', '20200529011308.json', '20200529011832.json', '20200529012357.json', '20200529012921.json', '20200529013445.json', '20200529014010.json', '20200529014534.json', '20200529015058.json', '20200529015622.json', '20200529020147.json', '20200529020711.json', '20200529021237.json', '20200529021801.json', '20200529022325.json', '20200529022850.json', '20200529023414.json', '20200529023938.json', '20200529024503.json', '20200529025027.json', '20200529025551.json', '20200529030116.json', '20200529030640.json', '20200529031205.json', '20200529031729.json', '20200529032256.json', '20200529032823.json', '20200529033349.json', '20200529033918.json', '20200529034444.json', '20200529035008.json', '20200529035533.json', '20200529040057.json', '20200529040622.json', '20200529041147.json', '20200529041711.json', '20200529042236.json', '20200529042800.json', '20200529043325.json', '20200529043849.json', '20200529044414.json', '20200529044939.json', '20200529045503.json', '20200529050027.json', '20200529050552.json', '20200529051117.json', '20200529051641.json', '20200529052206.json', '20200529052730.json', '20200529053255.json', '20200529053819.json', '20200529054344.json', '20200529054908.json', '20200529055433.json', '20200529055958.json', '20200529060522.json', '20200529061047.json', '20200529061611.json', '20200529062136.json', '20200529062700.json', '20200529063225.json', '20200529063749.json', '20200529064314.json', '20200529064838.json', '20200529065403.json', '20200529065927.json', '20200529070452.json', '20200529071016.json', '20200529071540.json', '20200529072105.json', '20200529072629.json', '20200529073154.json', '20200529073718.json', '20200529074242.json', '20200529074807.json', '20200529075331.json', '20200529075855.json', '20200529080420.json', '20200529080944.json', '20200529081508.json', '20200529082033.json', '20200529082557.json', '20200529083122.json', '20200529083646.json', '20200529084210.json', '20200529084735.json', '20200529085259.json', '20200529085824.json', '20200529090348.json', '20200529090913.json', '20200529091437.json', '20200529092001.json', '20200529092526.json', '20200529093050.json', '20200529093614.json', '20200529094139.json', '20200529094703.json', '20200529095228.json', '20200529095752.json', '20200529100317.json', '20200529100841.json', '20200529101406.json', '20200529101930.json', '20200529102454.json', '20200529103019.json', '20200529103543.json']\n"
     ]
    }
   ],
   "source": [
    "# Python read all files from a directory in order\n",
    "import os\n",
    "\n",
    "f = open(\"./data/netflow_name.txt\", 'w')\n",
    "path = \"./data/netflow/\"\n",
    "files = os.listdir(path)\n",
    "files.sort()\n",
    "\n",
    "s = []\n",
    "\n",
    "for file_name in files:\n",
    "    if not os.path.isdir(path + file_name):\n",
    "        f_name = str(file_name)\n",
    "        print(f_name)\n",
    "        s.append(f_name)\n",
    "        f.write(f_name + '\\n')\n",
    "for i in s:\n",
    "    print(i)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protocol_to_number(protocol):\n",
    "    if protocol == 'TCP':\n",
    "        num = 1\n",
    "    elif protocol=='UDP':\n",
    "        num = 2\n",
    "    elif protocol == 'ssh':\n",
    "        num = 3\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create database: assetdb_ts\n",
      "1000133 records inserted successful。\n",
      "mysql consume time: [65.08331241900123]\n",
      "Write DataFrame\n",
      "                        srcaddr  srcport\n",
      "First                                   \n",
      "2020-05-28 14:07:31       32265     2770\n",
      "2020-05-28 14:07:31       34163    65021\n",
      "2020-05-28 14:07:31       40594    16787\n",
      "2020-05-28 14:07:31       41447    37220\n",
      "2020-05-28 14:07:31        4489     9037\n",
      "...                         ...      ...\n",
      "2020-05-28 14:07:31  1667444601     7066\n",
      "2020-05-28 14:07:31  1667447053    38140\n",
      "2020-05-28 14:07:31  1667451437    54016\n",
      "2020-05-28 14:07:31  1667454360    41382\n",
      "2020-05-28 14:07:31  1667434974    25293\n",
      "\n",
      "[1000000 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type 'int64' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-24fdd00db415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mclient_write_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_record\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'demo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'srcaddr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_precision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mclient_write_end_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0minf_write_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_write_end_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mclient_write_start_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/my_env/lib/python3.6/site-packages/influxdb/_dataframe_client.py\u001b[0m in \u001b[0;36mwrite_points\u001b[0;34m(self, dataframe, measurement, tags, tag_columns, field_columns, time_precision, database, retention_policy, batch_size, protocol, numeric_precision)\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mretention_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     protocol=protocol)\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/my_env/lib/python3.6/site-packages/influxdb/client.py\u001b[0m in \u001b[0;36mwrite_points\u001b[0;34m(self, points, time_precision, database, retention_policy, tags, batch_size, protocol, consistency)\u001b[0m\n\u001b[1;32m    597\u001b[0m                                   \u001b[0mretention_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretention_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                                   \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                                   consistency=consistency)\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/my_env/lib/python3.6/site-packages/influxdb/client.py\u001b[0m in \u001b[0;36m_write_points\u001b[0;34m(self, points, time_precision, database, retention_policy, tags, protocol, consistency)\u001b[0m\n\u001b[1;32m    674\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mexpected_response_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m204\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                 \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m             )\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/my_env/lib/python3.6/site-packages/influxdb/client.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data, params, expected_response_code, protocol)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mexpected_response_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_response_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         )\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/my_env/lib/python3.6/site-packages/influxdb/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, url, method, params, data, stream, expected_response_code, headers)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gzip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'int64' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "from influxdb import DataFrameClient\n",
    "\n",
    "# update 1 X 1000000 records at a time\n",
    "# Use List to collect the time of each insert operation for every 1 million records\n",
    "# Insert thread pool configuration\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  passwd=\"phpipamadmin\",\n",
    "  database=\"assetdb\"\n",
    ")\n",
    "\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "mycursor.execute(\"CREATE TABLE assetdb_main (srcaddr INT UNSIGNED, srcport MEDIUMINT UNSIGNED, first BIGINT UNSIGNED, last BIGINT UNSIGNED, protocol TINYINT UNSIGNED, flows SMALLINT UNSIGNED, packets MEDIUMINT UNSIGNED, bytes INT UNSIGNED, PRIMARY KEY(`srcaddr`)) \\\n",
    "ENGINE=InnoDB DEFAULT CHARSET=utf8 PARTITION BY HASH(srcaddr) partitions 64;\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Instantiate the connection to the InfluxDB client.\"\"\"\n",
    "user = 'root'\n",
    "password = '0318'\n",
    "dbname = 'assetdb_ts'\n",
    "protocol = 'line'\n",
    "\n",
    "client = DataFrameClient('localhost', 8086, user, password, dbname)\n",
    "print(\"Create database: \" + dbname)\n",
    "client.create_database(dbname)\n",
    "\n",
    "mysql_write_time = []\n",
    "inf_write_time = []\n",
    "time_records = []\n",
    "time_records_opt = []\n",
    "\n",
    "for json_name in s:\n",
    "#    mydb.reconnect()\n",
    "    mycursor = mydb.cursor()\n",
    "    sql_update = \"INSERT INTO assetdb_main (srcaddr, srcport, first, last, protocol, flows, packets, bytes) \\\n",
    "    VALUES (%s,%s,%s,%s,%s,%s,%s,%s) ON DUPLICATE KEY UPDATE `last` = VALUES(`last`), \\\n",
    "    `flows`= VALUES(`flows`) + `flows`, `packets`= VALUES(`packets`) + `packets`, `bytes`= VALUES(`bytes`) + `bytes`\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    flows = []\n",
    "    with open('data/netflow/' + json_name) as record:\n",
    "        # import json file\n",
    "        new_record = json.load(record)\n",
    "        \n",
    "        # sort all records by 'srcaddr' to improve query speed\n",
    "        #new_record_df.sort_values(\"srcaddr\", inplace=True)\n",
    "        new_record.sort(key=lambda k: (k.get('srcaddr', 0)))\n",
    "        \n",
    "        # begin to count the total time of writing records into mysql\n",
    "        mysql_write_start_time = time.perf_counter()\n",
    "        \n",
    "        # transform [{},{}] to [(),()] of data format\n",
    "        for i,flow in zip(range(1000000),new_record):\n",
    "            flow[\"srcaddr\"] = struct.unpack('!I', socket.inet_aton(flow[\"srcaddr\"]))[0]\n",
    "            new_record[i][\"srcaddr\"] = flow[\"srcaddr\"]\n",
    "            protocol = protocol_to_number(flow[\"protocol\"])\n",
    "            flow_one = (flow[\"srcaddr\"], flow[\"srcport\"], flow[\"First\"], flow[\"Last\"], protocol, flow[\"flows\"], flow[\"packets\"], flow[\"bytes\"])\n",
    "            flows.append(flow_one)\n",
    "            \n",
    "        # write records into mysql\n",
    "        mycursor.executemany(sql_update, flows)\n",
    "        print(mycursor.rowcount, \"records inserted successful。\")\n",
    "        mycursor.close()\n",
    "        \n",
    "        # commit updated table,数据表内容有更新，必须使用到该语句\n",
    "        mydb.commit()   \n",
    "        \n",
    "        # calculate the total time of writing records into mysql\n",
    "        mysql_write_end_time = time.perf_counter()\n",
    "        mysql_write_time.append(mysql_write_end_time - mysql_write_start_time)\n",
    "        print(\"mysql consume time:\",mysql_write_time)\n",
    "        \n",
    "        # Filter 'srcaddr', 'arcport', and 'First' from records, and transform list to DataFrame\n",
    "        new_record = pd.DataFrame(data=new_record, columns=['srcaddr','srcport','First'])\n",
    "        \n",
    "        #new_record[\"srcaddr\"] = new_record[\"srcaddr\"].map(lambda x: struct.unpack('!I', socket.inet_aton(x))[0])\n",
    "        \n",
    "        # convert the format of values in 'First' column to datetime  \n",
    "        new_record['First'] = pd.to_datetime(new_record['First'], format='%Y%m%d%H%M%S')\n",
    "        \n",
    "        # Set the 'First' as index\n",
    "        new_record.set_index('First', inplace=True)\n",
    "        \n",
    "        # write DataFrame into influxdb and count the total time\n",
    "        print(\"Write DataFrame\")\n",
    "        client_write_start_time = time.perf_counter()\n",
    "        print(new_record)\n",
    "        client.write_points(new_record, 'demo', tag_columns=['srcaddr'], time_precision='ms', batch_size=10000, protocol=protocol)\n",
    "        client_write_end_time = time.perf_counter()\n",
    "        inf_write_time.append(client_write_end_time - client_write_start_time)\n",
    "        print(\"Client Library Write: {inf_write}s\".format(inf_write))\n",
    "        \n",
    "        # calculate the total time of writing records into mysql and influxdb\n",
    "        end_time = time.time()\n",
    "        consume_time = end_time - start_time\n",
    "        time_records.append(consume_time)\n",
    "        print(consume_time) \n",
    "        consume_time = '{:0>2s}'.format(str(int(consume_time // 3600))) \\\n",
    "               + ':{:0>2s}'.format(str(int((consume_time // 60) % 60))) \\\n",
    "               + ':{:0>2s}'.format(str(int(consume_time % 60)))\n",
    "        time_records_opt.append(consume_time)\n",
    "        print(consume_time) \n",
    "        \n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete database: assetdb_ts\n"
     ]
    }
   ],
   "source": [
    "mycursor.execute(\"drop table assetdb_main;\")\n",
    "print(\"Delete database: \" + dbname)\n",
    "client.drop_database(dbname)\n",
    "\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'srcaddr': '57.216.112.001', 'srcport': 63975, 'First': 20200528140731, 'Last': 20200528140931, 'protocol': 'UDP', 'flows': 18, 'packets': 295, 'bytes': 482341}, {'srcaddr': '184.105.195.183', 'srcport': 38885, 'First': 20200528140731, 'Last': 20200528142533, 'protocol': 'TCP', 'flows': 3, 'packets': 112, 'bytes': 215847}, {'srcaddr': '125.202.46.124', 'srcport': 10110, 'First': 20200528140731, 'Last': 20200528142534, 'protocol': 'UDP', 'flows': 1, 'packets': 243, 'bytes': 115125}]\n",
      "184.105.195.183\n",
      "970485761\n",
      "3093939127\n",
      "2110402172\n",
      "[{'srcaddr': 970485761, 'srcport': 63975, 'First': 20200528140731, 'Last': 20200528140931, 'protocol': 'UDP', 'flows': 18, 'packets': 295, 'bytes': 482341}, {'srcaddr': 3093939127, 'srcport': 38885, 'First': 20200528140731, 'Last': 20200528142533, 'protocol': 'TCP', 'flows': 3, 'packets': 112, 'bytes': 215847}, {'srcaddr': 2110402172, 'srcport': 10110, 'First': 20200528140731, 'Last': 20200528142534, 'protocol': 'UDP', 'flows': 1, 'packets': 243, 'bytes': 115125}]\n"
     ]
    }
   ],
   "source": [
    "with open('data/test data/' + \"test_data.json\") as record:\n",
    "        # import json file\n",
    "        new_record = json.load(record)\n",
    "        print(new_record)\n",
    "        #print(type(new_record))\n",
    "        #new_record_df = pd.DataFrame(data=new_record)\n",
    "        flows = []\n",
    "        #print(new_record_df)\n",
    "\n",
    "        print(new_record[1][\"srcaddr\"])\n",
    "        \n",
    "        for i,flow in zip(range(3),new_record):\n",
    "            flow[\"srcaddr\"] = struct.unpack('!I', socket.inet_aton(flow[\"srcaddr\"]))[0]\n",
    "            new_record[i][\"srcaddr\"] = flow[\"srcaddr\"]\n",
    "            print(flow[\"srcaddr\"])\n",
    "        \n",
    "        print(new_record)\n",
    "\n",
    "\n",
    "        \n",
    "        #df = pd.DataFrame(data=new_record)\n",
    "        #print(df)\n",
    "        #print(type(df))\n",
    "        \n",
    "        #df_2 = df.loc[1].values\n",
    "        #print(df_2)\n",
    "        #print(type(df_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement multiply  thread optimize\n",
    "# thread 1 write records into asset main\n",
    "# thread 2 write records into asset time\n",
    "# thread 3 query and get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error:\n",
    "    When using dataframe to inset records in mysql, all data formats will be first converted to int64, this will waste storge space!\n",
    " example:         \n",
    "    for i in new_record_df.index:\n",
    "            print(tuple(new_record_df.loc[i,:].values))\n",
    "            flows.append(tuple(new_record_df.loc[i,:].values))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

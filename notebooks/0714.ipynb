{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aioinflux'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6c91d3aeef95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mV9TemplateNotRecognized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmysql_os\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMysqlOperation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfluxdb_os\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInsertRecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/package/influxdb_os.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maioinflux\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInfluxDBClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aioinflux'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Reference collector script for NetFlow v1, v5, and v9 Python package.\n",
    "This file belongs to https://github.com/bitkeks/python-netflow-v9-softflowd.\n",
    "Copyright 2016-2020 Dominik Pataky <software+pynetflow@dpataky.eu>\n",
    "Licensed under MIT License. See LICENSE.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import queue\n",
    "import socket\n",
    "import socketserver\n",
    "import threading\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "from package.ipfix import IPFIXTemplateNotRecognized\n",
    "#from package.utils import *\n",
    "from package.utils import UnknownExportVersion, parse_packet, flow_filter_v4, flow_filter_v6\n",
    "from package.v9 import V9TemplateNotRecognized\n",
    "from package.mysql_os import MysqlOperation\n",
    "from package.influxdb_os import InsertRecords\n",
    "\n",
    "\n",
    "RawPacket = namedtuple('RawPacket', ['ts', 'client', 'data'])\n",
    "ParsedPacket = namedtuple('ParsedPacket', ['ts', 'client', 'export'])\n",
    "\n",
    "# Amount of time to wait before dropping an undecodable ExportPacket\n",
    "PACKET_TIMEOUT = 60 * 60\n",
    "\n",
    "logger = logging.getLogger(\"netflow-collector\")\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "class QueuingRequestHandler(socketserver.BaseRequestHandler):\n",
    "    def handle(self):\n",
    "        data = self.request[0]  # get content, [1] would be the socket\n",
    "        self.server.queue.put(RawPacket(time.time(), self.client_address, data))\n",
    "        logger.debug(\n",
    "            \"Received %d bytes of data from %s\", len(data), self.client_address\n",
    "        )\n",
    "\n",
    "\n",
    "class QueuingUDPListener(socketserver.ThreadingUDPServer):\n",
    "    \"\"\"A threaded UDP server that adds a (time, data) tuple to a queue for\n",
    "    every request it sees\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, interface, queue):\n",
    "        self.queue = queue\n",
    "\n",
    "        # If IPv6 interface addresses are used, override the default AF_INET family\n",
    "        if \":\" in interface[0]:\n",
    "            self.address_family = socket.AF_INET6\n",
    "\n",
    "        super().__init__(interface, QueuingRequestHandler)\n",
    "\n",
    "\n",
    "class ThreadedNetFlowListener(threading.Thread):\n",
    "    \"\"\"A thread that listens for incoming NetFlow packets, processes them, and\n",
    "    makes them available to consumers.\n",
    "    - When initialized, will start listening for NetFlow packets on the provided\n",
    "      host and port and queuing them for processing.\n",
    "    - When started, will start processing and parsing queued packets.\n",
    "    - When stopped, will shut down the listener and stop processing.\n",
    "    - When joined, will wait for the listener to exit\n",
    "    For example, a simple script that outputs data until killed with CTRL+C:\n",
    "    >>> listener = ThreadedNetFlowListener('0.0.0.0', 2055)\n",
    "    >>> print(\"Listening for NetFlow packets\")\n",
    "    >>> listener.start() # start processing packets\n",
    "    >>> try:\n",
    "    ...     while True:\n",
    "    ...         ts, export = listener.get()\n",
    "    ...         print(\"Time: {}\".format(ts))\n",
    "    ...         for f in export.flows:\n",
    "    ...             print(\" - {IPV4_SRC_ADDR} sent data to {IPV4_DST_ADDR}\"\n",
    "    ...                   \"\".format(**f))\n",
    "    ... finally:\n",
    "    ...     print(\"Stopping...\")\n",
    "    ...     listener.stop()\n",
    "    ...     listener.join()\n",
    "    ...     print(\"Stopped!\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, host: str, port: int):\n",
    "        logger.info(\"Starting the NetFlow listener on {}:{}\".format(host, port))\n",
    "        self.output = queue.Queue()\n",
    "        self.input = queue.Queue()\n",
    "        self.server = QueuingUDPListener((host, port), self.input)\n",
    "        self.thread = threading.Thread(target=self.server.serve_forever)\n",
    "        self.thread.start()\n",
    "        self._shutdown = threading.Event()\n",
    "        super().__init__()\n",
    "\n",
    "    def get(self, block=True, timeout=None) -> ParsedPacket:\n",
    "        \"\"\"Get a processed flow.\n",
    "        If optional args 'block' is true and 'timeout' is None (the default),\n",
    "        block if necessary until a flow is available. If 'timeout' is\n",
    "        a non-negative number, it blocks at most 'timeout' seconds and raises\n",
    "        the queue.Empty exception if no flow was available within that time.\n",
    "        Otherwise ('block' is false), return a flow if one is immediately\n",
    "        available, else raise the queue.Empty exception ('timeout' is ignored\n",
    "        in that case).\n",
    "        \"\"\"\n",
    "        return self.output.get(block, timeout)\n",
    "\n",
    "    def run(self):\n",
    "        # Process packets from the queue\n",
    "        try:\n",
    "            templates = {\"netflow\": {}, \"ipfix\": {}}\n",
    "            to_retry = []\n",
    "            while not self._shutdown.is_set():\n",
    "                try:\n",
    "                    # 0.5s delay to limit CPU usage while waiting for new packets\n",
    "                    pkt = self.input.get(block=True, timeout=0.5)  # type: RawPacket\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # templates is passed as reference, updated in V9ExportPacket\n",
    "                    export = parse_packet(pkt.data, templates)\n",
    "                except UnknownExportVersion as e:\n",
    "                    logger.error(\"%s, ignoring the packet\", e)\n",
    "                    continue\n",
    "                except (V9TemplateNotRecognized, IPFIXTemplateNotRecognized):\n",
    "                    # TODO: differentiate between v9 and IPFIX, use separate to_retry lists\n",
    "                    if time.time() - pkt.ts > PACKET_TIMEOUT:\n",
    "                        logger.warning(\"Dropping an old and undecodable v9/IPFIX ExportPacket\")\n",
    "                    else:\n",
    "                        to_retry.append(pkt)\n",
    "                        logger.debug(\"Failed to decode a v9/IPFIX ExportPacket - will \"\n",
    "                                     \"re-attempt when a new template is discovered\")\n",
    "                    continue\n",
    "\n",
    "                if export.header.version == 10:\n",
    "                    logger.debug(\"Processed an IPFIX ExportPacket with length %d.\", export.header.length)\n",
    "                else:\n",
    "                    logger.debug(\"Processed a v%d ExportPacket with %d flows.\",\n",
    "                                 export.header.version, export.header.count)\n",
    "\n",
    "                # If any new templates were discovered, dump the unprocessable\n",
    "                # data back into the queue and try to decode them again\n",
    "                if export.header.version in [9, 10] and export.contains_new_templates and to_retry:\n",
    "                    logger.debug(\"Received new template(s)\")\n",
    "                    logger.debug(\"Will re-attempt to decode %d old v9/IPFIX ExportPackets\", len(to_retry))\n",
    "                    for p in to_retry:\n",
    "                        self.input.put(p)\n",
    "                    to_retry.clear()\n",
    "\n",
    "                self.output.put(ParsedPacket(pkt.ts, pkt.client, export))\n",
    "        finally:\n",
    "            # Only reached when while loop ends\n",
    "            self.server.shutdown()\n",
    "            self.server.server_close()\n",
    "\n",
    "    def stop(self):\n",
    "        logger.info(\"Shutting down the NetFlow listener\")\n",
    "        self._shutdown.set()\n",
    "\n",
    "    def join(self, timeout=None):\n",
    "        self.thread.join(timeout=timeout)\n",
    "        super().join(timeout=timeout)\n",
    "\n",
    "\n",
    "def get_export_packets(host: str, port: int) -> ParsedPacket:\n",
    "    \"\"\"A threaded generator that will yield ExportPacket objects until it is killed\n",
    "    \"\"\"\n",
    "    listener = ThreadedNetFlowListener(host, port)\n",
    "    listener.start()\n",
    "    try:\n",
    "        while True:\n",
    "            yield listener.get()\n",
    "    finally:\n",
    "        listener.stop()\n",
    "        listener.join()\n",
    "        \n",
    "async def main(*flows):\n",
    "    myTool = MysqlOperation()\n",
    "    await asyncio.gather(\n",
    "        \n",
    "        myTool.insertRecords(*flows)\n",
    "    )\n",
    "\n",
    "if __name__ == \"netflow.collector\":\n",
    "    logger.error(\"The collector is currently meant to be used as a CLI tool only.\")\n",
    "    logger.error(\"Use 'python3 -m netflow.collector -h' in your console for additional help.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # With every parsed flow a new line is appended to the output file. In previous versions, this was implemented\n",
    "        # by storing the whole data dict in memory and dumping it regularly onto disk. This was extremely fragile, as\n",
    "        # it a) consumed a lot of memory and CPU (dropping packets since storing one flow took longer than the arrival\n",
    "        # of the next flow) and b) broke the exported JSON file, if the collector crashed during the write process,\n",
    "        # rendering all collected flows during the runtime of the collector useless (the file contained one large JSON\n",
    "        # dict which represented the 'data' dict).\n",
    "\n",
    "        # In this new approach, each received flow is parsed as usual, but it gets appended to a gzipped file each time.\n",
    "        # All in all, this improves in three aspects:InnodbOperation\n",
    "        # 1. collected flow data is not stored in memory any more\n",
    "        # 2. received and parsed flows are persisted reliably\n",
    "        # 3. the disk usage of files with JSON and its full strings as keys is reduced by using gzipped files\n",
    "        # This also means that the files have to be handled differently, because they are gzipped and not formatted as\n",
    "        # one single big JSON dump, but rather many little JSON dumps, separated by line breaks.\n",
    "    flows = []\n",
    "        #for ts, client, export in get_export_packets(\"0.0.0.0\", 9996):\n",
    "    record = pd.read_csv(\"test.csv\")\n",
    "    print(record.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      218.146.20.61 2020-06-28 23:55:43.274 2020-06-29 00:08:50.735     6       63     6905       70\n",
      "0      108.82.154.57 2020-06-29 00:04:46.736 2020...                                                \n",
      "1     212.102.35.141 2020-06-29 00:07:17.380 2020...                                                \n",
      "2    104.214.230.139 2020-06-29 00:02:21.153 2020...                                                \n",
      "3       154.24.10.58 2020-06-29 00:04:11.048 2020...                                                \n",
      "4       18.196.98.21 2020-06-28 23:56:58.631 2020...                                                \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "record = pd.read_csv(\"test.csv\")\n",
    "print(record.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('   138.246.193.20 2020-06-28 23:59:58.701 2020-06-29 00:09:02.980    74       77     3299       48',)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"test.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = [tuple(row) for row in reader]\n",
    "\n",
    "print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('    108.82.154.57 2020-06-29 00:04:46.736 2020-06-29 00:08:30.961     9        9      360       12',)\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "# open file in read mode\n",
    "with open('test.csv', 'r') as read_obj:\n",
    "    # pass the file object to reader() to get the reader object\n",
    "    csv_reader = reader(read_obj)\n",
    "    # print(csv_reader)\n",
    "    # Get all rows of csv from csv_reader object as list of tuples\n",
    "    list_of_tuples = list(map(tuple, csv_reader))\n",
    "    # display all rows of csv\n",
    "    print(list_of_tuples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 srcaddr       first          last  flows  \\\n",
      "218.146.20.61   2020-06-28  23:55:43.274  2020-06-29  00:08:50.735      6   \n",
      "108.82.154.57   2020-06-29  00:04:46.736  2020-06-29  00:08:30.961      9   \n",
      "212.102.35.141  2020-06-29  00:07:17.380  2020-06-29  00:07:17.380      1   \n",
      "104.214.230.139 2020-06-29  00:02:21.153  2020-06-29  00:08:34.975     15   \n",
      "154.24.10.58    2020-06-29  00:04:11.048  2020-06-29  00:04:23.261      1   \n",
      "\n",
      "                            packets  bytes   bps  \n",
      "218.146.20.61   2020-06-28     63.0   6905    70  \n",
      "108.82.154.57   2020-06-29      9.0    360    12  \n",
      "212.102.35.141  2020-06-29      1.0    134     0  \n",
      "104.214.230.139 2020-06-29    150.0  88581  1895  \n",
      "154.24.10.58    2020-06-29      3.0    132    86  \n"
     ]
    }
   ],
   "source": [
    "import pandas as ps\n",
    "df4 = ps.read_csv('test.csv', delim_whitespace=True, names=['srcaddr', 'first', 'last', 'flows', 'packets', 'bytes', 'bps'])\n",
    "print(df4.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 298279: expected 1 fields, saw 6\\nSkipping line 298281: expected 1 fields, saw 3\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Src IP Addr Date first seen         Date last seen          Proto Flows  Packets    Bytes      bps\n",
      "0      218.146.20.61 2020-06-28 23:55:43.274 2020...                                                      \n",
      "1      108.82.154.57 2020-06-29 00:04:46.736 2020...                                                      \n",
      "2     212.102.35.141 2020-06-29 00:07:17.380 2020...                                                      \n",
      "3    104.214.230.139 2020-06-29 00:02:21.153 2020...                                                      \n",
      "4       154.24.10.58 2020-06-29 00:04:11.048 2020...                                                      \n"
     ]
    }
   ],
   "source": [
    "import pandas as ps\n",
    "df4 = ps.read_csv('test_header.csv', error_bad_lines = False)\n",
    "print(df4.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Src IP Addr Date first seen         Date last seen          Proto Flows  Packets    Bytes      bps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218.146.20.61 2020-06-28 23:55:43.274 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108.82.154.57 2020-06-29 00:04:46.736 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212.102.35.141 2020-06-29 00:07:17.380 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.214.230.139 2020-06-29 00:02:21.153 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.24.10.58 2020-06-29 00:04:11.048 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298273</th>\n",
       "      <td>179.36.147.72 2020-06-29 00:03:14.178 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298274</th>\n",
       "      <td>109.117.39.134 2020-06-29 00:08:12.797 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298275</th>\n",
       "      <td>129.187.61.156 2020-06-29 00:02:34.798 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298276</th>\n",
       "      <td>105.213.96.92 2020-06-29 00:04:22.291 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298277</th>\n",
       "      <td>Time window: 2020-06-28 22:49:17 - 2020-06-29 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298278 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Src IP Addr Date first seen         Date last seen          Proto Flows  Packets    Bytes      bps\n",
       "0           218.146.20.61 2020-06-28 23:55:43.274 2020...                                                      \n",
       "1           108.82.154.57 2020-06-29 00:04:46.736 2020...                                                      \n",
       "2          212.102.35.141 2020-06-29 00:07:17.380 2020...                                                      \n",
       "3         104.214.230.139 2020-06-29 00:02:21.153 2020...                                                      \n",
       "4            154.24.10.58 2020-06-29 00:04:11.048 2020...                                                      \n",
       "...                                                   ...                                                      \n",
       "298273      179.36.147.72 2020-06-29 00:03:14.178 2020...                                                      \n",
       "298274     109.117.39.134 2020-06-29 00:08:12.797 2020...                                                      \n",
       "298275     129.187.61.156 2020-06-29 00:02:34.798 2020...                                                      \n",
       "298276      105.213.96.92 2020-06-29 00:04:22.291 2020...                                                      \n",
       "298277  Time window: 2020-06-28 22:49:17 - 2020-06-29 ...                                                      \n",
       "\n",
       "[298278 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head(298278)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop([len(df4)-1],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Src IP Addr Date first seen         Date last seen          Proto Flows  Packets    Bytes      bps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218.146.20.61 2020-06-28 23:55:43.274 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108.82.154.57 2020-06-29 00:04:46.736 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212.102.35.141 2020-06-29 00:07:17.380 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.214.230.139 2020-06-29 00:02:21.153 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.24.10.58 2020-06-29 00:04:11.048 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298272</th>\n",
       "      <td>95.91.234.111 2020-06-29 00:04:12.096 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298273</th>\n",
       "      <td>179.36.147.72 2020-06-29 00:03:14.178 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298274</th>\n",
       "      <td>109.117.39.134 2020-06-29 00:08:12.797 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298275</th>\n",
       "      <td>129.187.61.156 2020-06-29 00:02:34.798 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298276</th>\n",
       "      <td>105.213.96.92 2020-06-29 00:04:22.291 2020...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298277 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Src IP Addr Date first seen         Date last seen          Proto Flows  Packets    Bytes      bps\n",
       "0           218.146.20.61 2020-06-28 23:55:43.274 2020...                                                      \n",
       "1           108.82.154.57 2020-06-29 00:04:46.736 2020...                                                      \n",
       "2          212.102.35.141 2020-06-29 00:07:17.380 2020...                                                      \n",
       "3         104.214.230.139 2020-06-29 00:02:21.153 2020...                                                      \n",
       "4            154.24.10.58 2020-06-29 00:04:11.048 2020...                                                      \n",
       "...                                                   ...                                                      \n",
       "298272      95.91.234.111 2020-06-29 00:04:12.096 2020...                                                      \n",
       "298273      179.36.147.72 2020-06-29 00:03:14.178 2020...                                                      \n",
       "298274     109.117.39.134 2020-06-29 00:08:12.797 2020...                                                      \n",
       "298275     129.187.61.156 2020-06-29 00:02:34.798 2020...                                                      \n",
       "298276      105.213.96.92 2020-06-29 00:04:22.291 2020...                                                      \n",
       "\n",
       "[298277 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5941b1f9ce72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/environments/py37-venv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'names'"
     ]
    }
   ],
   "source": [
    "df4.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to connect db! \n",
      "succeed to connect db!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhao/environments/py37-venv/lib/python3.7/site-packages/aiomysql/cursors.py:239: Warning: Data truncated for column 'bps' at row 1\n",
      "  await self._query(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "execute insert cost: \n",
      "insert res: 10000\n",
      "close pool!\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "import socket, struct\n",
    "import re\n",
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import queue\n",
    "import socketserver\n",
    "import threading\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "from package.ipfix import IPFIXTemplateNotRecognized\n",
    "#from package.utils import *\n",
    "from package.utils import UnknownExportVersion, parse_packet, flow_filter_v4, flow_filter_v6\n",
    "from package.v9 import V9TemplateNotRecognized\n",
    "from package.mysql_os import MysqlOperation\n",
    "#from package.influxdb_os import InsertRecords\n",
    "\n",
    "myTool = MysqlOperation()\n",
    "\n",
    "# open file in read mode\n",
    "with open('test.csv', 'r') as read_obj:\n",
    "    flow_list = []\n",
    "    row = ()\n",
    "    for line in read_obj:\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        row = line.split(' ')\n",
    "        \n",
    "        row = list(filter(None, row))\n",
    "\n",
    "        #row = row[1].join(row[2])\n",
    "        if(len(row) == 9):\n",
    "            (srcaddr, first_Y_M_D, first_H_M_S,last_Y_M_D, last_H_M_S, flows, packets, byte, bps) = row\n",
    "            try:\n",
    "                socket.inet_aton(srcaddr)\n",
    "                srcaddr = struct.unpack('!L', socket.inet_aton(srcaddr))[0]\n",
    "            except socket.error:\n",
    "                continue\n",
    "                # srcaddr = struct.unpack('!QQ', socket.inet_pton(socket.AF_INET6, srcaddr))[0]\n",
    "            first = first_Y_M_D + first_H_M_S\n",
    "            first = re.split(\"-|:|\\.\",first)\n",
    "            first = int(\"\".join(first[0:5]))\n",
    "            last = last_Y_M_D+last_H_M_S\n",
    "            last = re.split(\"-|:|\\.\",last)\n",
    "            last = int(\"\".join(last[0:5]))\n",
    "            flows = int(flows)\n",
    "            packets = int(float(packets))\n",
    "            byte = int(float(byte))\n",
    "            bps = int(float(bps))\n",
    "            row = (srcaddr, first, last, flows, packets, byte, bps) \n",
    "        elif(len(row) == 10):\n",
    "            (srcaddr, first_Y_M_D, first_H_M_S,last_Y_M_D, last_H_M_S, flows, packets, byte, byte_unit, bps) = row\n",
    "            try:\n",
    "                socket.inet_aton(srcaddr)\n",
    "                srcaddr = struct.unpack('!L', socket.inet_aton(srcaddr))[0]\n",
    "            except socket.error:\n",
    "                continue\n",
    "                # srcaddr = struct.unpack('!QQ', socket.inet_pton(socket.AF_INET6, srcaddr))[0]\n",
    "            first = first_Y_M_D + first_H_M_S\n",
    "            first = re.split(\"-|:|\\.\",first)\n",
    "            first = int(\"\".join(first[0:5]))\n",
    "            last = last_Y_M_D+last_H_M_S\n",
    "            last = re.split(\"-|:|\\.\",last)\n",
    "            last = int(\"\".join(last[0:5]))\n",
    "            flows = int(flows)\n",
    "            packets = int(float(packets))\n",
    "            if(byte_unit == 'M'):\n",
    "                byte = int(float(byte)) * 10**6\n",
    "            elif(byte_unit == 'G'):\n",
    "                byte = int(float(byte)) * 10**9\n",
    "            elif(bps == 'M'): \n",
    "                bps = byte_unit * 10**6\n",
    "            elif(bps == 'G'): \n",
    "                bps = byte_unit * 10**9\n",
    "            row = (srcaddr, first, last, flows, packets, byte, bps) \n",
    "        elif(len(row) == 11):\n",
    "            (srcaddr, first_Y_M_D, first_H_M_S,last_Y_M_D, last_H_M_S, flows, packets, byte, byte_unit, bps, bps_unit) = row\n",
    "            try:\n",
    "                socket.inet_aton(srcaddr)\n",
    "                srcaddr = struct.unpack('!L', socket.inet_aton(srcaddr))[0]\n",
    "            except socket.error:\n",
    "                continue\n",
    "                # srcaddr = struct.unpack('!QQ', socket.inet_pton(socket.AF_INET6, srcaddr))[0]\n",
    "            first = first_Y_M_D + first_H_M_S\n",
    "            first = re.split(\"-|:|\\.\",first)\n",
    "            first = int(\"\".join(first[0:5]))\n",
    "            last = last_Y_M_D+last_H_M_S\n",
    "            last = re.split(\"-|:|\\.\",last)\n",
    "            last = int(\"\".join(last[0:5]))\n",
    "            flows = int(flows)\n",
    "            if(byte == 'M'):\n",
    "                packets = int(float(packets)) * 10**6\n",
    "                if(bps == 'M'):\n",
    "                    byte = int(float(byte_unit)) * 10**6\n",
    "                elif(bps == 'G'):\n",
    "                    byte = int(float(byte_unit)) * 10**9\n",
    "                bps = bps_unit                    \n",
    "            elif(byte == 'G'):\n",
    "                packets = int(float(packets)) * 10**9\n",
    "                if(bps == 'M'):\n",
    "                    byte = int(float(byte_unit)) * 10**6\n",
    "                elif(bps == 'G'):\n",
    "                    byte = int(float(byte_unit)) * 10**9\n",
    "                bps = bps_unit   \n",
    "            else:    \n",
    "                packets = int(float(packets))\n",
    "                if(byte_unit == 'M'):\n",
    "                    byte = int(float(byte)) * 10**6\n",
    "                elif(byte_unit == 'G'):\n",
    "                    byte = int(float(byte)) * 10**9\n",
    "                if(bps_unit == 'M'):\n",
    "                    bps = int(float(bps)) * 10**6\n",
    "                elif(bps_unit == 'M'):\n",
    "                    bps = int(float(bps)) * 10**9\n",
    "            row = (srcaddr, first, last, flows, packets, byte, bps) \n",
    "        elif(len(row) == 12):\n",
    "            (srcaddr, first_Y_M_D, first_H_M_S,last_Y_M_D, last_H_M_S, flows, packets, packets_unit, byte, byte_unit, bps, bps_unit) = row\n",
    "            try:\n",
    "                socket.inet_aton(srcaddr)\n",
    "                srcaddr = struct.unpack('!L', socket.inet_aton(srcaddr))[0]\n",
    "            except socket.error:\n",
    "                continue\n",
    "                # srcaddr = struct.unpack('!QQ', socket.inet_pton(socket.AF_INET6, srcaddr))[0]\n",
    "            first = first_Y_M_D + first_H_M_S\n",
    "            first = re.split(\"-|:|\\.\",first)\n",
    "            first = int(\"\".join(first[0:5]))\n",
    "            last = last_Y_M_D+last_H_M_S\n",
    "            last = re.split(\"-|:|\\.\",last)\n",
    "            last = int(\"\".join(last[0:5]))\n",
    "            flows = int(float(flows))\n",
    "            if(packets_unit == 'M'):\n",
    "                packets = int(float(packets)) * 10**6\n",
    "            elif(packets_unit == 'G'):\n",
    "                packets = int(float(packets)) * 10**9\n",
    "            if(byte_unit == 'M'):\n",
    "                byte = int(float(byte)) * 10**6\n",
    "            elif(byte_unit == 'G'):\n",
    "                byte = int(float(byte)) * 10**9\n",
    "            if(bps_unit == 'M'):\n",
    "                bps = int(float(bps)) * 10**6\n",
    "            elif(bps_unit == 'M'):\n",
    "                bps = int(float(bps)) * 10**9\n",
    "            row = (srcaddr, first, last, flows, packets, byte, bps) \n",
    "        flow_list.append(row)\n",
    "        if(len(flow_list) == 10000):\n",
    "            \n",
    "            asyncio.run(asyncio.gather(myTool.insertRecords(*flow_list)))\n",
    "            flow_list = []\n",
    "            # asyncio.run(asyncio.gather(myTool.insertRecords(*flow_list), InsertRecords(*flow_list)))\n",
    "            # print(flow_list)\n",
    "            # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"2020-06-2823:55:43.274\"\n",
    "test = re.split(\"-|:|\\.\",first_1)\n",
    "test = \"\".join(test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20200628235543'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xda\\x92\\x14='"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = '218.146.20.61'\n",
    "socket.inet_aton(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "ipv6 = '2804:a8..8::1392'\n",
    "try:\n",
    "    socket.inet_aton(ipv6)\n",
    "    print(\"1\")\n",
    "except socket.error:\n",
    "    print(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "illegal IP address string passed to inet_pton",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-eeb53cfa0050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mipv6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2804:a8..8::1392'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhexlify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minet_pton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_INET6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipv6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: illegal IP address string passed to inet_pton"
     ]
    }
   ],
   "source": [
    "from binascii import hexlify\n",
    "import ipaddress\n",
    "ipv6 = '2804:a8..8::1392'\n",
    "\n",
    "int(hexlify(socket.inet_pton(socket.AF_INET6, ipv6)), 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aioinflux\n",
      "  Using cached aioinflux-0.9.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: aiohttp>=3.0 in ./py37-venv/lib/python3.7/site-packages (from aioinflux) (3.6.2)\n",
      "Collecting ciso8601\n",
      "  Using cached ciso8601-2.1.3.tar.gz (15 kB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./py37-venv/lib/python3.7/site-packages (from aiohttp>=3.0->aioinflux) (1.4.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./py37-venv/lib/python3.7/site-packages (from aiohttp>=3.0->aioinflux) (19.3.0)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in ./py37-venv/lib/python3.7/site-packages (from aiohttp>=3.0->aioinflux) (4.7.6)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in ./py37-venv/lib/python3.7/site-packages (from aiohttp>=3.0->aioinflux) (3.0.4)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in ./py37-venv/lib/python3.7/site-packages (from aiohttp>=3.0->aioinflux) (3.0.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./py37-venv/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.0->aioinflux) (2.10)\n",
      "Building wheels for collected packages: ciso8601\n",
      "  Building wheel for ciso8601 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ciso8601: filename=ciso8601-2.1.3-cp37-cp37m-linux_x86_64.whl size=29068 sha256=36c8f014d621f47b7c676c15a21bf3f6358e022fc8020dcf855cdf7f554b1e98\n",
      "  Stored in directory: /home/yuhao/.cache/pip/wheels/96/0f/89/b1c8e876a1c8ebf41226adea77b12c4540ffc323006124954d\n",
      "Successfully built ciso8601\n",
      "Installing collected packages: ciso8601, aioinflux\n",
      "Successfully installed aioinflux-0.9.0 ciso8601-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install aioinflux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ciso8601\n",
      "  Using cached ciso8601-2.1.3.tar.gz (15 kB)\n",
      "Building wheels for collected packages: ciso8601\n",
      "  Building wheel for ciso8601 (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /home/yuhao/environments/py37-venv/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-5lcwteus/ciso8601/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-5lcwteus/ciso8601/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-blww2gdu\n",
      "       cwd: /tmp/pip-install-5lcwteus/ciso8601/\n",
      "  Complete output (18 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  package init file 'ciso8601/__init__.py' not found (or not a regular file)\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.7\n",
      "  creating build/lib.linux-x86_64-3.7/ciso8601\n",
      "  copying ciso8601/__init__.pyi -> build/lib.linux-x86_64-3.7/ciso8601\n",
      "  copying ciso8601/py.typed -> build/lib.linux-x86_64-3.7/ciso8601\n",
      "  running build_ext\n",
      "  building 'ciso8601' extension\n",
      "  creating build/temp.linux-x86_64-3.7\n",
      "  x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DCISO8601_VERSION=2.1.3 -I/home/yuhao/environments/py37-venv/include -I/usr/include/python3.7m -c module.c -o build/temp.linux-x86_64-3.7/module.o\n",
      "  module.c:1:10: fatal error: Python.h: No such file or directory\n",
      "   #include <Python.h>\n",
      "            ^~~~~~~~~~\n",
      "  compilation terminated.\n",
      "  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for ciso8601\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for ciso8601\n",
      "Failed to build ciso8601\n",
      "Installing collected packages: ciso8601\n",
      "    Running setup.py install for ciso8601 ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /home/yuhao/environments/py37-venv/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-5lcwteus/ciso8601/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-5lcwteus/ciso8601/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-0ss9x1g8/install-record.txt --single-version-externally-managed --compile --install-headers /home/yuhao/environments/py37-venv/include/site/python3.7/ciso8601\n",
      "         cwd: /tmp/pip-install-5lcwteus/ciso8601/\n",
      "    Complete output (18 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    package init file 'ciso8601/__init__.py' not found (or not a regular file)\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-3.7\n",
      "    creating build/lib.linux-x86_64-3.7/ciso8601\n",
      "    copying ciso8601/__init__.pyi -> build/lib.linux-x86_64-3.7/ciso8601\n",
      "    copying ciso8601/py.typed -> build/lib.linux-x86_64-3.7/ciso8601\n",
      "    running build_ext\n",
      "    building 'ciso8601' extension\n",
      "    creating build/temp.linux-x86_64-3.7\n",
      "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DCISO8601_VERSION=2.1.3 -I/home/yuhao/environments/py37-venv/include -I/usr/include/python3.7m -c module.c -o build/temp.linux-x86_64-3.7/module.o\n",
      "    module.c:1:10: fatal error: Python.h: No such file or directory\n",
      "     #include <Python.h>\n",
      "              ^~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /home/yuhao/environments/py37-venv/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-5lcwteus/ciso8601/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-5lcwteus/ciso8601/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-0ss9x1g8/install-record.txt --single-version-externally-managed --compile --install-headers /home/yuhao/environments/py37-venv/include/site/python3.7/ciso8601 Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aioinflux'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6c91d3aeef95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mV9TemplateNotRecognized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmysql_os\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMysqlOperation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfluxdb_os\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInsertRecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/package/influxdb_os.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maioinflux\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInfluxDBClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aioinflux'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Reference collector script for NetFlow v1, v5, and v9 Python package.\n",
    "This file belongs to https://github.com/bitkeks/python-netflow-v9-softflowd.\n",
    "Copyright 2016-2020 Dominik Pataky <software+pynetflow@dpataky.eu>\n",
    "Licensed under MIT License. See LICENSE.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import queue\n",
    "import socket\n",
    "import socketserver\n",
    "import threading\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "from package.ipfix import IPFIXTemplateNotRecognized\n",
    "#from package.utils import *\n",
    "from package.utils import UnknownExportVersion, parse_packet, flow_filter_v4, flow_filter_v6\n",
    "from package.v9 import V9TemplateNotRecognized\n",
    "from package.mysql_os import MysqlOperation\n",
    "from package.influxdb_os import InsertRecords\n",
    "\n",
    "\n",
    "RawPacket = namedtuple('RawPacket', ['ts', 'client', 'data'])\n",
    "ParsedPacket = namedtuple('ParsedPacket', ['ts', 'client', 'export'])\n",
    "\n",
    "# Amount of time to wait before dropping an undecodable ExportPacket\n",
    "PACKET_TIMEOUT = 60 * 60\n",
    "\n",
    "logger = logging.getLogger(\"netflow-collector\")\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "class QueuingRequestHandler(socketserver.BaseRequestHandler):\n",
    "    def handle(self):\n",
    "        data = self.request[0]  # get content, [1] would be the socket\n",
    "        self.server.queue.put(RawPacket(time.time(), self.client_address, data))\n",
    "        logger.debug(\n",
    "            \"Received %d bytes of data from %s\", len(data), self.client_address\n",
    "        )\n",
    "\n",
    "\n",
    "class QueuingUDPListener(socketserver.ThreadingUDPServer):\n",
    "    \"\"\"A threaded UDP server that adds a (time, data) tuple to a queue for\n",
    "    every request it sees\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, interface, queue):\n",
    "        self.queue = queue\n",
    "\n",
    "        # If IPv6 interface addresses are used, override the default AF_INET family\n",
    "        if \":\" in interface[0]:\n",
    "            self.address_family = socket.AF_INET6\n",
    "\n",
    "        super().__init__(interface, QueuingRequestHandler)\n",
    "\n",
    "\n",
    "class ThreadedNetFlowListener(threading.Thread):\n",
    "    \"\"\"A thread that listens for incoming NetFlow packets, processes them, and\n",
    "    makes them available to consumers.\n",
    "    - When initialized, will start listening for NetFlow packets on the provided\n",
    "      host and port and queuing them for processing.\n",
    "    - When started, will start processing and parsing queued packets.\n",
    "    - When stopped, will shut down the listener and stop processing.\n",
    "    - When joined, will wait for the listener to exit\n",
    "    For example, a simple script that outputs data until killed with CTRL+C:\n",
    "    >>> listener = ThreadedNetFlowListener('0.0.0.0', 2055)\n",
    "    >>> print(\"Listening for NetFlow packets\")\n",
    "    >>> listener.start() # start processing packets\n",
    "    >>> try:\n",
    "    ...     while True:\n",
    "    ...         ts, export = listener.get()\n",
    "    ...         print(\"Time: {}\".format(ts))\n",
    "    ...         for f in export.flows:\n",
    "    ...             print(\" - {IPV4_SRC_ADDR} sent data to {IPV4_DST_ADDR}\"\n",
    "    ...                   \"\".format(**f))\n",
    "    ... finally:\n",
    "    ...     print(\"Stopping...\")\n",
    "    ...     listener.stop()\n",
    "    ...     listener.join()\n",
    "    ...     print(\"Stopped!\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, host: str, port: int):\n",
    "        logger.info(\"Starting the NetFlow listener on {}:{}\".format(host, port))\n",
    "        self.output = queue.Queue()\n",
    "        self.input = queue.Queue()\n",
    "        self.server = QueuingUDPListener((host, port), self.input)\n",
    "        self.thread = threading.Thread(target=self.server.serve_forever)\n",
    "        self.thread.start()\n",
    "        self._shutdown = threading.Event()\n",
    "        super().__init__()\n",
    "\n",
    "    def get(self, block=True, timeout=None) -> ParsedPacket:\n",
    "        \"\"\"Get a processed flow.\n",
    "        If optional args 'block' is true and 'timeout' is None (the default),\n",
    "        block if necessary until a flow is available. If 'timeout' is\n",
    "        a non-negative number, it blocks at most 'timeout' seconds and raises\n",
    "        the queue.Empty exception if no flow was available within that time.\n",
    "        Otherwise ('block' is false), return a flow if one is immediately\n",
    "        available, else raise the queue.Empty exception ('timeout' is ignored\n",
    "        in that case).\n",
    "        \"\"\"\n",
    "        return self.output.get(block, timeout)\n",
    "\n",
    "    def run(self):\n",
    "        # Process packets from the queue\n",
    "        try:\n",
    "            templates = {\"netflow\": {}, \"ipfix\": {}}\n",
    "            to_retry = []\n",
    "            while not self._shutdown.is_set():\n",
    "                try:\n",
    "                    # 0.5s delay to limit CPU usage while waiting for new packets\n",
    "                    pkt = self.input.get(block=True, timeout=0.5)  # type: RawPacket\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # templates is passed as reference, updated in V9ExportPacket\n",
    "                    export = parse_packet(pkt.data, templates)\n",
    "                except UnknownExportVersion as e:\n",
    "                    logger.error(\"%s, ignoring the packet\", e)\n",
    "                    continue\n",
    "                except (V9TemplateNotRecognized, IPFIXTemplateNotRecognized):\n",
    "                    # TODO: differentiate between v9 and IPFIX, use separate to_retry lists\n",
    "                    if time.time() - pkt.ts > PACKET_TIMEOUT:\n",
    "                        logger.warning(\"Dropping an old and undecodable v9/IPFIX ExportPacket\")\n",
    "                    else:\n",
    "                        to_retry.append(pkt)\n",
    "                        logger.debug(\"Failed to decode a v9/IPFIX ExportPacket - will \"\n",
    "                                     \"re-attempt when a new template is discovered\")\n",
    "                    continue\n",
    "\n",
    "                if export.header.version == 10:\n",
    "                    logger.debug(\"Processed an IPFIX ExportPacket with length %d.\", export.header.length)\n",
    "                else:\n",
    "                    logger.debug(\"Processed a v%d ExportPacket with %d flows.\",\n",
    "                                 export.header.version, export.header.count)\n",
    "\n",
    "                # If any new templates were discovered, dump the unprocessable\n",
    "                # data back into the queue and try to decode them again\n",
    "                if export.header.version in [9, 10] and export.contains_new_templates and to_retry:\n",
    "                    logger.debug(\"Received new template(s)\")\n",
    "                    logger.debug(\"Will re-attempt to decode %d old v9/IPFIX ExportPackets\", len(to_retry))\n",
    "                    for p in to_retry:\n",
    "                        self.input.put(p)\n",
    "                    to_retry.clear()\n",
    "\n",
    "                self.output.put(ParsedPacket(pkt.ts, pkt.client, export))\n",
    "        finally:\n",
    "            # Only reached when while loop ends\n",
    "            self.server.shutdown()\n",
    "            self.server.server_close()\n",
    "\n",
    "    def stop(self):\n",
    "        logger.info(\"Shutting down the NetFlow listener\")\n",
    "        self._shutdown.set()\n",
    "\n",
    "    def join(self, timeout=None):\n",
    "        self.thread.join(timeout=timeout)\n",
    "        super().join(timeout=timeout)\n",
    "\n",
    "\n",
    "def get_export_packets(host: str, port: int) -> ParsedPacket:\n",
    "    \"\"\"A threaded generator that will yield ExportPacket objects until it is killed\n",
    "    \"\"\"\n",
    "    listener = ThreadedNetFlowListener(host, port)\n",
    "    listener.start()\n",
    "    try:\n",
    "        while True:\n",
    "            yield listener.get()\n",
    "    finally:\n",
    "        listener.stop()\n",
    "        listener.join()\n",
    "        \n",
    "async def main(*flows):\n",
    "    myTool = MysqlOperation()\n",
    "    await asyncio.gather(\n",
    "        \n",
    "        myTool.insertRecords(*flows)\n",
    "    )\n",
    "\n",
    "if __name__ == \"netflow.collector\":\n",
    "    logger.error(\"The collector is currently meant to be used as a CLI tool only.\")\n",
    "    logger.error(\"Use 'python3 -m netflow.collector -h' in your console for additional help.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # With every parsed flow a new line is appended to the output file. In previous versions, this was implemented\n",
    "        # by storing the whole data dict in memory and dumping it regularly onto disk. This was extremely fragile, as\n",
    "        # it a) consumed a lot of memory and CPU (dropping packets since storing one flow took longer than the arrival\n",
    "        # of the next flow) and b) broke the exported JSON file, if the collector crashed during the write process,\n",
    "        # rendering all collected flows during the runtime of the collector useless (the file contained one large JSON\n",
    "        # dict which represented the 'data' dict).\n",
    "\n",
    "        # In this new approach, each received flow is parsed as usual, but it gets appended to a gzipped file each time.\n",
    "        # All in all, this improves in three aspects:InnodbOperation\n",
    "        # 1. collected flow data is not stored in memory any more\n",
    "        # 2. received and parsed flows are persisted reliably\n",
    "        # 3. the disk usage of files with JSON and its full strings as keys is reduced by using gzipped files\n",
    "        # This also means that the files have to be handled differently, because they are gzipped and not formatted as\n",
    "        # one single big JSON dump, but rather many little JSON dumps, separated by line breaks.\n",
    "    flows = []\n",
    "        #for ts, client, export in get_export_packets(\"0.0.0.0\", 9996):\n",
    "    record = pd.read_csv(\"test.csv\")\n",
    "    print(record.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      218.146.20.61 2020-06-28 23:55:43.274 2020-06-29 00:08:50.735     6       63     6905       70\n",
      "0      108.82.154.57 2020-06-29 00:04:46.736 2020...                                                \n",
      "1     212.102.35.141 2020-06-29 00:07:17.380 2020...                                                \n",
      "2    104.214.230.139 2020-06-29 00:02:21.153 2020...                                                \n",
      "3       154.24.10.58 2020-06-29 00:04:11.048 2020...                                                \n",
      "4       18.196.98.21 2020-06-28 23:56:58.631 2020...                                                \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "record = pd.read_csv(\"test.csv\")\n",
    "print(record.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('   138.246.193.20 2020-06-28 23:59:58.701 2020-06-29 00:09:02.980    74       77     3299       48',)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"test.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = [tuple(row) for row in reader]\n",
    "\n",
    "print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('    108.82.154.57 2020-06-29 00:04:46.736 2020-06-29 00:08:30.961     9        9      360       12',)\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "# open file in read mode\n",
    "with open('test.csv', 'r') as read_obj:\n",
    "    # pass the file object to reader() to get the reader object\n",
    "    csv_reader = reader(read_obj)\n",
    "    # print(csv_reader)\n",
    "    # Get all rows of csv from csv_reader object as list of tuples\n",
    "    list_of_tuples = list(map(tuple, csv_reader))\n",
    "    # display all rows of csv\n",
    "    print(list_of_tuples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 srcaddr       first          last  flows  \\\n",
      "218.146.20.61   2020-06-28  23:55:43.274  2020-06-29  00:08:50.735      6   \n",
      "108.82.154.57   2020-06-29  00:04:46.736  2020-06-29  00:08:30.961      9   \n",
      "212.102.35.141  2020-06-29  00:07:17.380  2020-06-29  00:07:17.380      1   \n",
      "104.214.230.139 2020-06-29  00:02:21.153  2020-06-29  00:08:34.975     15   \n",
      "154.24.10.58    2020-06-29  00:04:11.048  2020-06-29  00:04:23.261      1   \n",
      "\n",
      "                            packets  bytes   bps  \n",
      "218.146.20.61   2020-06-28     63.0   6905    70  \n",
      "108.82.154.57   2020-06-29      9.0    360    12  \n",
      "212.102.35.141  2020-06-29      1.0    134     0  \n",
      "104.214.230.139 2020-06-29    150.0  88581  1895  \n",
      "154.24.10.58    2020-06-29      3.0    132    86  \n"
     ]
    }
   ],
   "source": [
    "import pandas as ps\n",
    "df4 = ps.read_csv('test.csv', delim_whitespace=True, names=['srcaddr', 'first', 'last', 'flows', 'packets', 'bytes', 'bps'])\n",
    "print(df4.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 298279: expected 1 fields, saw 6\\nSkipping line 298281: expected 1 fields, saw 3\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Src IP Addr Date first seen         Date last seen          Proto Flows  Packets    Bytes      bps\n",
      "0      218.146.20.61 2020-06-28 23:55:43.274 2020...                                                      \n",
      "1      108.82.154.57 2020-06-29 00:04:46.736 2020...                                                      \n",
      "2     212.102.35.141 2020-06-29 00:07:17.380 2020...                                                      \n",
      "3    104.214.230.139 2020-06-29 00:02:21.153 2020...                                                      \n",
      "4       154.24.10.58 2020-06-29 00:04:11.048 2020...                                                      \n"
     ]
    }
   ],
   "source": [
    "import pandas as ps\n",
    "df4 = ps.read_csv('test_header.csv', error_bad_lines = False)\n",
    "print(df4.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Src IP Addr Date first seen         Date last seen          Proto Flows  Packets    Bytes      bps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218.146.20.61 2020-06-28 23:55:43.274 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108.82.154.57 2020-06-29 00:04:46.736 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212.102.35.141 2020-06-29 00:07:17.380 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.214.230.139 2020-06-29 00:02:21.153 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.24.10.58 2020-06-29 00:04:11.048 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298273</th>\n",
       "      <td>179.36.147.72 2020-06-29 00:03:14.178 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298274</th>\n",
       "      <td>109.117.39.134 2020-06-29 00:08:12.797 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298275</th>\n",
       "      <td>129.187.61.156 2020-06-29 00:02:34.798 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298276</th>\n",
       "      <td>105.213.96.92 2020-06-29 00:04:22.291 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298277</th>\n",
       "      <td>Time window: 2020-06-28 22:49:17 - 2020-06-29 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298278 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Src IP Addr Date first seen         Date last seen          Proto Flows  Packets    Bytes      bps\n",
       "0           218.146.20.61 2020-06-28 23:55:43.274 2020...                                                      \n",
       "1           108.82.154.57 2020-06-29 00:04:46.736 2020...                                                      \n",
       "2          212.102.35.141 2020-06-29 00:07:17.380 2020...                                                      \n",
       "3         104.214.230.139 2020-06-29 00:02:21.153 2020...                                                      \n",
       "4            154.24.10.58 2020-06-29 00:04:11.048 2020...                                                      \n",
       "...                                                   ...                                                      \n",
       "298273      179.36.147.72 2020-06-29 00:03:14.178 2020...                                                      \n",
       "298274     109.117.39.134 2020-06-29 00:08:12.797 2020...                                                      \n",
       "298275     129.187.61.156 2020-06-29 00:02:34.798 2020...                                                      \n",
       "298276      105.213.96.92 2020-06-29 00:04:22.291 2020...                                                      \n",
       "298277  Time window: 2020-06-28 22:49:17 - 2020-06-29 ...                                                      \n",
       "\n",
       "[298278 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head(298278)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop([len(df4)-1],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Src IP Addr Date first seen         Date last seen          Proto Flows  Packets    Bytes      bps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218.146.20.61 2020-06-28 23:55:43.274 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108.82.154.57 2020-06-29 00:04:46.736 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212.102.35.141 2020-06-29 00:07:17.380 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.214.230.139 2020-06-29 00:02:21.153 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.24.10.58 2020-06-29 00:04:11.048 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298272</th>\n",
       "      <td>95.91.234.111 2020-06-29 00:04:12.096 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298273</th>\n",
       "      <td>179.36.147.72 2020-06-29 00:03:14.178 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298274</th>\n",
       "      <td>109.117.39.134 2020-06-29 00:08:12.797 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298275</th>\n",
       "      <td>129.187.61.156 2020-06-29 00:02:34.798 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298276</th>\n",
       "      <td>105.213.96.92 2020-06-29 00:04:22.291 2020...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298277 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Src IP Addr Date first seen         Date last seen          Proto Flows  Packets    Bytes      bps\n",
       "0           218.146.20.61 2020-06-28 23:55:43.274 2020...                                                      \n",
       "1           108.82.154.57 2020-06-29 00:04:46.736 2020...                                                      \n",
       "2          212.102.35.141 2020-06-29 00:07:17.380 2020...                                                      \n",
       "3         104.214.230.139 2020-06-29 00:02:21.153 2020...                                                      \n",
       "4            154.24.10.58 2020-06-29 00:04:11.048 2020...                                                      \n",
       "...                                                   ...                                                      \n",
       "298272      95.91.234.111 2020-06-29 00:04:12.096 2020...                                                      \n",
       "298273      179.36.147.72 2020-06-29 00:03:14.178 2020...                                                      \n",
       "298274     109.117.39.134 2020-06-29 00:08:12.797 2020...                                                      \n",
       "298275     129.187.61.156 2020-06-29 00:02:34.798 2020...                                                      \n",
       "298276      105.213.96.92 2020-06-29 00:04:22.291 2020...                                                      \n",
       "\n",
       "[298277 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5941b1f9ce72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/environments/py37-venv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'names'"
     ]
    }
   ],
   "source": [
    "df4.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to connect db! \n",
      "succeed to connect db!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhao/environments/py37-venv/lib/python3.7/site-packages/aiomysql/cursors.py:239: Warning: Data truncated for column 'bps' at row 1\n",
      "  await self._query(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write DataFrame\n",
      "execute insert cost: \n",
      "insert res: 100000\n",
      "Write DataFrame\n",
      "execute insert cost: \n",
      "insert res: 100000\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "import socket, struct\n",
    "import re\n",
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import queue\n",
    "import socketserver\n",
    "import threading\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "from package.ipfix import IPFIXTemplateNotRecognized\n",
    "#from package.utils import *\n",
    "from package.utils import UnknownExportVersion, parse_packet, flow_filter_v4, flow_filter_v6\n",
    "from package.v9 import V9TemplateNotRecognized\n",
    "from package.mysql_os import MysqlOperation\n",
    "from package.influxdb_os import InsertRecords\n",
    "\n",
    "myTool = MysqlOperation()\n",
    "\n",
    "def re_combine(line):\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        row = line.split(' ')\n",
    "        row = list(filter(None, row))\n",
    "        return row\n",
    "\n",
    "# open file in read mode\n",
    "# insert records to mysql and influxdb\n",
    "\n",
    "def InsertOperation(path_main, path_ts):\n",
    "    with open(path_main, 'r') as read_obj, open(path_ts, 'r') as read_obj_time:\n",
    "        flow_list = []\n",
    "        flow_time_list = []\n",
    "        row = ()\n",
    "        for line,line_time in zip(read_obj, read_obj_time):\n",
    "\n",
    "            row = re_combine(line)\n",
    "            row_time = re_combine(line_time)\n",
    "            #print(row)\n",
    "            #print(row_time)\n",
    "            #row = row[1].join(row[2])\n",
    "            if(len(row) == 9):\n",
    "                (srcaddr, first_Y_M_D, first_H_M_S,last_Y_M_D, last_H_M_S, flows, packets, byte, bps) = row\n",
    "                try:\n",
    "                    socket.inet_aton(srcaddr)\n",
    "                    srcaddr = struct.unpack('!L', socket.inet_aton(srcaddr))[0]\n",
    "                except socket.error:\n",
    "                    continue\n",
    "                    # srcaddr = struct.unpack('!QQ', socket.inet_pton(socket.AF_INET6, srcaddr))[0]\n",
    "                first = first_Y_M_D + first_H_M_S\n",
    "                first = re.split(\"-|:|\\.\",first)\n",
    "                first = int(\"\".join(first[0:5]))\n",
    "                last = last_Y_M_D+last_H_M_S\n",
    "                last = re.split(\"-|:|\\.\",last)\n",
    "                last = int(\"\".join(last[0:5]))\n",
    "                flows = int(flows)\n",
    "                packets = int(float(packets))\n",
    "                byte = int(float(byte))\n",
    "                bps = int(float(bps))\n",
    "                row = (srcaddr, first, last, flows, packets, byte, bps) \n",
    "            elif(len(row) == 10):\n",
    "                (srcaddr, first_Y_M_D, first_H_M_S,last_Y_M_D, last_H_M_S, flows, packets, byte, byte_unit, bps) = row\n",
    "                try:\n",
    "                    socket.inet_aton(srcaddr)\n",
    "                    srcaddr = struct.unpack('!L', socket.inet_aton(srcaddr))[0]\n",
    "                except socket.error:\n",
    "                    continue\n",
    "                    # srcaddr = struct.unpack('!QQ', socket.inet_pton(socket.AF_INET6, srcaddr))[0]\n",
    "                first = first_Y_M_D + first_H_M_S\n",
    "                first = re.split(\"-|:|\\.\",first)\n",
    "                first = int(\"\".join(first[0:5]))\n",
    "                last = last_Y_M_D+last_H_M_S\n",
    "                last = re.split(\"-|:|\\.\",last)\n",
    "                last = int(\"\".join(last[0:5]))\n",
    "                flows = int(flows)\n",
    "                packets = int(float(packets))\n",
    "                if(byte_unit == 'M'):\n",
    "                    byte = int(float(byte)) * 10**6\n",
    "                elif(byte_unit == 'G'):\n",
    "                    byte = int(float(byte)) * 10**9\n",
    "                elif(bps == 'M'): \n",
    "                    bps = byte_unit * 10**6\n",
    "                elif(bps == 'G'): \n",
    "                    bps = byte_unit * 10**9\n",
    "                row = (srcaddr, first, last, flows, packets, byte, bps) \n",
    "            elif(len(row) == 11):\n",
    "                (srcaddr, first_Y_M_D, first_H_M_S,last_Y_M_D, last_H_M_S, flows, packets, byte, byte_unit, bps, bps_unit) = row\n",
    "                try:\n",
    "                    socket.inet_aton(srcaddr)\n",
    "                    srcaddr = struct.unpack('!L', socket.inet_aton(srcaddr))[0]\n",
    "                except socket.error:\n",
    "                    continue\n",
    "                    # srcaddr = struct.unpack('!QQ', socket.inet_pton(socket.AF_INET6, srcaddr))[0]\n",
    "                first = first_Y_M_D + first_H_M_S\n",
    "                first = re.split(\"-|:|\\.\",first)\n",
    "                first = int(\"\".join(first[0:5]))\n",
    "                last = last_Y_M_D+last_H_M_S\n",
    "                last = re.split(\"-|:|\\.\",last)\n",
    "                last = int(\"\".join(last[0:5]))\n",
    "                flows = int(flows)\n",
    "                if(byte == 'M'):\n",
    "                    packets = int(float(packets)) * 10**6\n",
    "                    if(bps == 'M'):\n",
    "                        byte = int(float(byte_unit)) * 10**6\n",
    "                    elif(bps == 'G'):\n",
    "                        byte = int(float(byte_unit)) * 10**9\n",
    "                    bps = bps_unit                    \n",
    "                elif(byte == 'G'):\n",
    "                    packets = int(float(packets)) * 10**9\n",
    "                    if(bps == 'M'):\n",
    "                        byte = int(float(byte_unit)) * 10**6\n",
    "                    elif(bps == 'G'):\n",
    "                        byte = int(float(byte_unit)) * 10**9\n",
    "                    bps = bps_unit   \n",
    "                else:    \n",
    "                    packets = int(float(packets))\n",
    "                    if(byte_unit == 'M'):\n",
    "                        byte = int(float(byte)) * 10**6\n",
    "                    elif(byte_unit == 'G'):\n",
    "                        byte = int(float(byte)) * 10**9\n",
    "                    if(bps_unit == 'M'):\n",
    "                        bps = int(float(bps)) * 10**6\n",
    "                    elif(bps_unit == 'M'):\n",
    "                        bps = int(float(bps)) * 10**9\n",
    "                row = (srcaddr, first, last, flows, packets, byte, bps) \n",
    "            elif(len(row) == 12):\n",
    "                (srcaddr, first_Y_M_D, first_H_M_S,last_Y_M_D, last_H_M_S, flows, packets, packets_unit, byte, byte_unit, bps, bps_unit) = row\n",
    "                try:\n",
    "                    socket.inet_aton(srcaddr)\n",
    "                    srcaddr = struct.unpack('!L', socket.inet_aton(srcaddr))[0]\n",
    "                except socket.error:\n",
    "                    continue\n",
    "                    # srcaddr = struct.unpack('!QQ', socket.inet_pton(socket.AF_INET6, srcaddr))[0]\n",
    "                first = first_Y_M_D + first_H_M_S\n",
    "                first = re.split(\"-|:|\\.\",first)\n",
    "                first = int(\"\".join(first[0:5]))\n",
    "                last = last_Y_M_D+last_H_M_S\n",
    "                last = re.split(\"-|:|\\.\",last)\n",
    "                last = int(\"\".join(last[0:5]))\n",
    "                flows = int(float(flows))\n",
    "                if(packets_unit == 'M'):\n",
    "                    packets = int(float(packets)) * 10**6\n",
    "                elif(packets_unit == 'G'):\n",
    "                    packets = int(float(packets)) * 10**9\n",
    "                if(byte_unit == 'M'):\n",
    "                    byte = int(float(byte)) * 10**6\n",
    "                elif(byte_unit == 'G'):\n",
    "                    byte = int(float(byte)) * 10**9\n",
    "                if(bps_unit == 'M'):\n",
    "                    bps = int(float(bps)) * 10**6\n",
    "                elif(bps_unit == 'M'):\n",
    "                    bps = int(float(bps)) * 10**9\n",
    "                row = (srcaddr, first, last, flows, packets, byte, bps) \n",
    "            flow_list.append(row)\n",
    "            (last_Y_M_D_time, last_H_M_S_time, srcaddr, srcport, proto) = row_time\n",
    "            last = last_Y_M_D_time + last_H_M_S_time\n",
    "            last = re.split(\"-|:|\\.\",last)\n",
    "            last = int(\"\".join(last[0:5]))\n",
    "            row_time = (last, srcaddr, srcport, proto)\n",
    "            flow_time_list.append(row_time)\n",
    "            if(len(flow_list) == 100000):\n",
    "\n",
    "                asyncio.run(asyncio.gather(myTool.insertRecords(*flow_list), InsertRecords(*flow_time_list)))\n",
    "                flow_list = []\n",
    "                flow_time_list = []\n",
    "                # asyncio.run(asyncio.gather(myTool.insertRecords(*flow_list), InsertRecords(*flow_list)))\n",
    "                # print(flow_list)\n",
    "                # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueuingReader():\n",
    "    def __init__(self, interface, queue):\n",
    "        self.queue = queue\n",
    "        \n",
    "class ThreadedNfdumpReader(threading.Thread):\n",
    "    def __init__(self):\n",
    "        logger.info(\"Starting the NetFlow files reader.\")\n",
    "        threading.Thread.__init__(self)\n",
    "        self.output = queue.Queue()\n",
    "        self.input = queue.Queue()\n",
    "        self.thread = threading.Thread(target = self.server)\n",
    "\n",
    "def get_nfdump_file():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d2ebd3ef3629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2020-06-2823:55:43.274\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-|:|\\.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfirst_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "test = \"2020-06-2823:55:43.274\"\n",
    "test = re.split(\"-|:|\\.\",first_1)\n",
    "test = \"\".join(test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-16 01:53:51.074204\n",
      "2020-07-16 01:53:51 cpu:8.2%, mem:54.5%\n",
      "2020-07-16 01:53:51 bytessent=5630473660, bytesrecv=8629396253\n",
      "2020-07-16 01:53:51.076180\n",
      "2020-07-16 01:53:52 bytessent=5630482257, bytesrecv=8629404850\n",
      "2020-07-16 01:53:53 bytessent=5630484385, bytesrecv=8629407038\n",
      "2020-07-16 01:53:54 cpu:10.1%, mem:54.5%\n",
      "2020-07-16 01:53:54 bytessent=5630486513, bytesrecv=8629409226\n",
      "2020-07-16 01:53:55 bytessent=5630488975, bytesrecv=8629411748\n",
      "2020-07-16 01:53:56 bytessent=5630491103, bytesrecv=8629413936\n",
      "2020-07-16 01:53:57 cpu:8.9%, mem:54.6%\n",
      "2020-07-16 01:53:57 bytessent=5630493352, bytesrecv=8629416124\n",
      "2020-07-16 01:53:58 bytessent=5630495685, bytesrecv=8629418524\n",
      "2020-07-16 01:53:59 bytessent=5630497813, bytesrecv=8629420712\n",
      "2020-07-16 01:54:00 cpu:5.5%, mem:54.5%\n",
      "2020-07-16 01:54:00 bytessent=5630499941, bytesrecv=8629422900\n",
      "2020-07-16 01:54:01 bytessent=5630502295, bytesrecv=8629425278\n",
      "2020-07-16 01:54:02 bytessent=5630504665, bytesrecv=8629427406\n",
      "2020-07-16 01:54:03 cpu:5.5%, mem:54.5%\n",
      "2020-07-16 01:54:03 bytessent=5630507363, bytesrecv=8629429922\n",
      "2020-07-16 01:54:04 bytessent=5630510002, bytesrecv=8629432351\n",
      "2020-07-16 01:54:05 bytessent=5630512372, bytesrecv=8629434539\n",
      "2020-07-16 01:54:06 cpu:4.6%, mem:54.5%\n",
      "2020-07-16 01:54:06 bytessent=5630514718, bytesrecv=8629436879\n",
      "2020-07-16 01:54:07 bytessent=5630517090, bytesrecv=8629439190\n",
      "2020-07-16 01:54:08 bytessent=5630519382, bytesrecv=8629441521\n",
      "2020-07-16 01:54:09 cpu:7.1%, mem:54.5%\n",
      "2020-07-16 01:54:09 bytessent=5630521510, bytesrecv=8629443828\n",
      "2020-07-16 01:54:10 bytessent=5630523969, bytesrecv=8629446287\n",
      "2020-07-16 01:54:11 bytessent=5630526097, bytesrecv=8629448475\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from threading import Timer\n",
    "import psutil\n",
    "import time\n",
    "import datetime\n",
    "#logfile：监测信息写入文件\n",
    "def MonitorSystem(logfile = None):\n",
    "    #获取cpu使用情况\n",
    "    cpuper = psutil.cpu_percent()\n",
    "    #获取内存使用情况：系统内存大小，使用内存，有效内存，内存使用率\n",
    "    mem = psutil.virtual_memory()\n",
    "    #内存使用率\n",
    "    memper = mem.percent\n",
    "    #获取当前时间\n",
    "    now = datetime.datetime.now()\n",
    "    ts = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    line = f'{ts} cpu:{cpuper}%, mem:{memper}%'\n",
    "    print(line)\n",
    "    if logfile:\n",
    "        logfile.write(line)\n",
    "    Timer(3, MonitorSystem).start()\n",
    "        \n",
    "def MonitorNetWork(logfile = None):\n",
    "    #获取网络收信息\n",
    "    netinfo = psutil.net_io_counters()\n",
    "    #获取当前时间\n",
    "    now = datetime.datetime.now()\n",
    "    ts = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    line = f'{ts} bytessent={netinfo.bytes_sent}, bytesrecv={netinfo.bytes_recv}'\n",
    "    print(line)\n",
    "    if logfile:\n",
    "        logfile.write(line)\n",
    "    Timer(1, MonitorNetWork).start()\n",
    "\n",
    "\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "MonitorSystem()\n",
    "MonitorNetWork()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inQueue working!\n",
      "start to connect db! \n",
      "succeed to connect db!\n",
      "start to connect influxdb! \n",
      "outQueue working!\n",
      "nfcapd.202006290010\n",
      "outQueue working!\n",
      "nfcapd.202006290015\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "0.940648\n",
      "x:  ['20:24:03']\n",
      "y:  [0.940648]\n",
      "20:24:03 execute insert cost: 0.940648seconds\n",
      "Write DataFrame\n",
      "inQueue working!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhao/environments/py37-venv/lib/python3.7/site-packages/aiomysql/cursors.py:239: Warning: Data truncated for column 'bps' at row 1\n",
      "  await self._query(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "34.118636\n",
      "20:24:36 execute insert cost: 34.118636seconds\n",
      "x:  ['20:24:36']\n",
      "y:  [34.118636]\n",
      "insert res: 100000\n",
      "0.707769\n",
      "x:  ['20:24:03', '20:24:39']\n",
      "y:  [0.940648, 0.707769]\n",
      "20:24:39 execute insert cost: 0.707769seconds\n",
      "Write DataFrame\n",
      "inQueue working!\n",
      "16.466886\n",
      "20:24:55 execute insert cost: 16.466886seconds\n",
      "x:  ['20:24:36', '20:24:55']\n",
      "y:  [34.118636, 16.466886]\n",
      "insert res: 100000\n",
      "outQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n",
      "inQueue working!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from queue import Queue\n",
    "from threading import Thread,Timer\n",
    "from package.insert_operation import InsertOperation\n",
    "\n",
    "# Monitor and manage the specifical folder, \n",
    "class MonitorFolder():\n",
    "    # Read all files name into a list\n",
    "    def __init__(self, path):\n",
    "        self.q = Queue()\n",
    "        self.path = path\n",
    "        files = os.listdir(path)\n",
    "        files.sort()\n",
    "        self.flag = files[-1]\n",
    "        for i in files:\n",
    "            self.q.put(i)\n",
    "\n",
    "# TODO change 10 seconds to 5 minutes\n",
    "\n",
    "    # put all files name into a queue\n",
    "    # every 10 seconds scan the folder once\n",
    "    def inQueue(self):\n",
    "        files = os.listdir(self.path)\n",
    "        files.sort()\n",
    "        for i in files:\n",
    "            if(i > self.flag):\n",
    "                self.q.put(i)\n",
    "        self.flag = files[-1]\n",
    "        print(\"inQueue working!\")\n",
    "        Timer(10, self.inQueue).start()\n",
    "    \n",
    "    # get files name from the queue\n",
    "    def outQueue(self):\n",
    "        while True:\n",
    "            print(\"outQueue working!\")\n",
    "            yield self.q.get(block=True, timeout=None)\n",
    "\n",
    "path = './data/test'\n",
    "reader = MonitorFolder(path)\n",
    "reader.inQueue()\n",
    "src_path = './data/test/'\n",
    "dst_path = './data/csv/'\n",
    "worker = InsertOperation()\n",
    "for i in reader.outQueue():\n",
    "        print(i)\n",
    "        (file_format, name) = i.split('.')\n",
    "        file_main_path = dst_path + name + '_main.csv'\n",
    "        file_ts_path = dst_path + name + '_ts.csv'\n",
    "        os.system('nfdump -r ' + src_path + i + ' -A srcip,srcport,proto -q -o \\'fmt: %te %sa %sp %pr\\' > ' + file_ts_path)\n",
    "        os.system('nfdump -r ' + src_path + i + ' -A srcip -q -o \\'fmt: %sa %ts %te %fl %pkt %byt %bps\\' > ' + file_main_path)\n",
    "\n",
    "        worker.InsertToDB(file_main_path,file_ts_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.8e-05"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "start_time = datetime.datetime.now() \n",
    "end_time = datetime.datetime.now()\n",
    "consume_time = end_time - start_time\n",
    "#consume_time = consume_time\n",
    "#end_time = consume_time.strftime(\"%H:%M:%S\")\n",
    "#time_records.append(consume_time)\n",
    "consume_time.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "def plot_time_tracking(y, x, plot_name):\n",
    "        plt.plot(x, y)\n",
    "        plt.savefig(plot_name)\n",
    "        \n",
    "plot_time_tracking([0,0],[1,2],\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'datetime.datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d2a4efbaaab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_time2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current Time =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'datetime.datetime'"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M\")\n",
    "\n",
    "hours = 5\n",
    "hours_added = timedelta(hours = hours)\n",
    "current_time2 = datetime.now() + hours_added\n",
    "\n",
    "\n",
    "time = int(current_time2) - int(current_time)\n",
    "print(\"Current Time =\", time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18-05', '18-05', '18-05', '18-05']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "x = []\n",
    "curr = datetime.datetime.now().strftime(\"%H-%M\")\n",
    "for i in range(len([2,4,6, 8])):\n",
    "    x.append(curr)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to connect db! \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot run the event loop while another loop is running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e7e17cfa8d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m pool = loop.run_until_complete(aiomysql.create_pool(host='127.0.0.1', port=3306,\n\u001b[1;32m      7\u001b[0m                                     \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'root'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'phpipamadmin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                     db='assetdb', charset='utf8'))\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_until_complete_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_task\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_forever\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             raise RuntimeError(\n\u001b[0;32m--> 524\u001b[0;31m                 'Cannot run the event loop while another loop is running')\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_coroutine_origin_tracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ident\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot run the event loop while another loop is running"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiomysql\n",
    "\n",
    "print(\"start to connect db! \")\n",
    "loop = asyncio.new_event_loop()\n",
    "pool = loop.run_until_complete(aiomysql.create_pool(host='127.0.0.1', port=3306,\n",
    "                                    user='root', password='phpipamadmin',\n",
    "                                    db='assetdb', charset='utf8'))\n",
    "conn = loop.run_until_complete(self.pool.acquire())\n",
    "cur = loop.run_until_complete(conn.cursor())\n",
    "loop.run_until_complete(cur.execute(\"DROP TABLE assetdb_main\"))\n",
    "loop.run_until_complete(cur.execute(\"CREATE TABLE assetdb_main (srcaddr INT UNSIGNED, first BIGINT UNSIGNED, last BIGINT UNSIGNED, flows INT UNSIGNED, packets BIGINT UNSIGNED, bytes BIGINT UNSIGNED, bps BIGINT UNSIGNED, PRIMARY KEY(`srcaddr`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 PARTITION BY HASH(srcaddr) partitions 100;\"))\n",
    "\n",
    "print(\"succeed to connect db!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('path_main', 'r') as read_obj,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread,Timer\n",
    "\n",
    "import os\n",
    "from queue import Queue\n",
    "\n",
    "q = Queue()\n",
    "\n",
    "class MonitorFolder(Thread):\n",
    "    def __init__(self, path):\n",
    "        self.q = Queue()\n",
    "        self.path = path\n",
    "        files = os.listdir(path)\n",
    "        files.sort()\n",
    "        self.flag = files[-1]\n",
    "        for i in files:\n",
    "            self.q.put(i)\n",
    "        super().__init__()\n",
    "            \n",
    "    def inQueue(self):\n",
    "        files = os.listdir(self.path)\n",
    "        files.sort()\n",
    "        for i in files:\n",
    "            if(i > self.flag):\n",
    "                self.q.put(i)\n",
    "                print(i)\n",
    "        self.flag = files[-1]\n",
    "        print(\"inQueue working!\")\n",
    "        print(files[-1])\n",
    "        Timer(10, self.inQueue).start()\n",
    "        \n",
    "    def outQueue(self):\n",
    "        while True:\n",
    "            print(\"outQueue working!\")\n",
    "            print(self.q.get(block=True, timeout=None))\n",
    "\n",
    "        \n",
    "\n",
    "path = './data/netflow'\n",
    "reader = MonitorFolder(path)\n",
    "reader.inQueue()\n",
    "t2 = Thread(target=reader.outQueue)\n",
    "t2.start()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reader.outQueue():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xda\\x92\\x14='"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = '218.146.20.61'\n",
    "socket.inet_aton(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "ipv6 = '2804:a8..8::1392'\n",
    "try:\n",
    "    socket.inet_aton(ipv6)\n",
    "    print(\"1\")\n",
    "except socket.error:\n",
    "    print(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "illegal IP address string passed to inet_pton",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-eeb53cfa0050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mipv6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2804:a8..8::1392'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhexlify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minet_pton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_INET6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipv6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: illegal IP address string passed to inet_pton"
     ]
    }
   ],
   "source": [
    "from binascii import hexlify\n",
    "import ipaddress\n",
    "ipv6 = '2804:a8..8::1392'\n",
    "\n",
    "int(hexlify(socket.inet_pton(socket.AF_INET6, ipv6)), 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200528142940.json\n",
      "20200528143504.json\n"
     ]
    }
   ],
   "source": [
    "test = ['20200528140758.json', '20200528141325.json', '20200528141851.json', '20200528142416.json', '20200528142940.json', '20200528143504.json']\n",
    "fla = '20200528142416.json'\n",
    "for i in test:\n",
    "    if(i > fla):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202006290005\n"
     ]
    }
   ],
   "source": [
    "i = 'nfcapd.202006290005'\n",
    "(file_format, name) = i.split('.')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
